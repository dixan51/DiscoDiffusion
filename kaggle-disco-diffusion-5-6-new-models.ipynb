{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Set Up","metadata":{"_uuid":"21cbee9f-2c57-45cf-aed7-e0198fb2a528","_cell_guid":"0f42d35f-396f-4e1d-b857-18b2ad65b105","id":"SetupTop","trusted":true}},{"cell_type":"code","source":"#@title 1.1 Check GPU Status\nimport subprocess\nsimple_nvidia_smi_display = True#@param {type:\"boolean\"}\nif simple_nvidia_smi_display:\n    #!nvidia-smi\n    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(nvidiasmi_output)\nelse:\n    #!nvidia-smi -i 0 -e 0\n    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(nvidiasmi_output)\n    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(nvidiasmi_ecc_note)","metadata":{"execution":{"iopub.status.busy":"2022-10-18T23:57:43.865795Z","iopub.execute_input":"2022-10-18T23:57:43.866318Z","iopub.status.idle":"2022-10-18T23:57:44.046630Z","shell.execute_reply.started":"2022-10-18T23:57:43.866276Z","shell.execute_reply":"2022-10-18T23:57:44.043749Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-c0fbe6d4-343a-7367-00d4-fbabe702f935)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title 1.2 Prepare Folders\nimport subprocess, os, sys, ipykernel\nfrom torchvision.datasets.utils import download_url\n\ndef gitclone(url):\n    res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(res)\n\ndef pipi(modulestr):\n    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(res)\n\ndef pipie(modulestr):\n    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    print(res)\n\n#def wget(url, outputdir):\n#    res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n#    print(res)\n\nroot_path = os.getcwd()\n\nimport os\ndef createPath(filepath):\n    os.makedirs(filepath, exist_ok=True)\n\ninitDirPath = f'{root_path}/init_images'\ncreatePath(initDirPath)\noutDirPath = f'{root_path}/images_out'\n#outDirPath = f'{root_path}/' # bad workaround for data_subdirs not refreshing in draft session, switched to FileLinks\ncreatePath(outDirPath)\n\nmodel_path = f'{root_path}/models'\ncreatePath(model_path)\n\n# libraries = f'{root_path}/libraries'\n# createPath(libraries)","metadata":{"_uuid":"9038d3e8-ff3b-447c-88f1-dcb51550ae3d","_cell_guid":"ab814c4c-e0ca-449c-8fc8-20b9e4322877","id":"PrepFolders","execution":{"iopub.status.busy":"2022-10-18T23:57:44.049544Z","iopub.execute_input":"2022-10-18T23:57:44.050330Z","iopub.status.idle":"2022-10-18T23:57:44.066070Z","shell.execute_reply.started":"2022-10-18T23:57:44.050278Z","shell.execute_reply":"2022-10-18T23:57:44.065044Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#@title ### 1.3 Install, import dependencies and set up runtime devices\n\nimport pathlib, shutil, os, sys\n\n#@markdown Check this if you want to use CPU\nuseCPU = False #@param {type:\"boolean\"}\n# If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\nos.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n\nPROJECT_DIR = os.path.abspath(os.getcwd())\nUSE_ADABINS = True\n\nroot_path = os.getcwd()\nmodel_path = f'{root_path}/models'\n\nmultipip_res = subprocess.run(['pip', 'install', 'lpips', 'datetime', 'timm', 'ftfy', 'einops', 'pytorch-lightning', 'omegaconf'], stdout=subprocess.PIPE).stdout.decode('utf-8')\nprint(multipip_res)\n\ntry:\n    from CLIP import clip\nexcept:\n    if not os.path.exists(\"CLIP\"):\n        gitclone(\"https://github.com/openai/CLIP\")\n    sys.path.append(f'{PROJECT_DIR}/CLIP')\n\ntry:\n    import open_clip\nexcept:\n    if not os.path.exists(\"open_clip/src\"):\n        gitclone(\"https://github.com/mlfoundations/open_clip.git\")\n    sys.path.append(f'{PROJECT_DIR}/open_clip/src')\n    import open_clip\n\ntry:\n    from guided_diffusion.script_util import create_model_and_diffusion\nexcept:\n    if not os.path.exists(\"guided-diffusion\"):\n        gitclone(\"https://github.com/kostarion/guided-diffusion\")\n    sys.path.append(f'{PROJECT_DIR}/guided-diffusion')\n\ntry:\n    from resize_right import resize\nexcept:\n    if not os.path.exists(\"ResizeRight\"):\n        gitclone(\"https://github.com/assafshocher/ResizeRight.git\")\n    sys.path.append(f'{PROJECT_DIR}/ResizeRight')\n\ntry:\n    import py3d_tools\nexcept:\n    if not os.path.exists('pytorch3d-lite'):\n        gitclone(\"https://github.com/MSFTserver/pytorch3d-lite.git\")\n    sys.path.append(f'{PROJECT_DIR}/pytorch3d-lite')\n\ntry:\n    from midas.dpt_depth import DPTDepthModel\nexcept:\n    if not os.path.exists('MiDaS'):\n        gitclone(\"https://github.com/isl-org/MiDaS.git\")\n    if not os.path.exists('MiDaS/midas_utils.py'):\n        shutil.move('MiDaS/utils.py', 'MiDaS/midas_utils.py')\n    if not os.path.exists(f'{model_path}/dpt_large-midas-2f21e586.pt'):\n        download_url(\"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\", model_path)\n    sys.path.append(f'{PROJECT_DIR}/MiDaS')\n\ntry:\n    sys.path.append(PROJECT_DIR)\n    import disco_xform_utils as dxf\nexcept:\n    if not os.path.exists(\"disco-diffusion\"):\n        gitclone(\"https://github.com/alembics/disco-diffusion.git\")\n    if not os.path.exists('disco_xform_utils.py'):\n        shutil.move('disco-diffusion/disco_xform_utils.py', 'disco_xform_utils.py')\n    sys.path.append(PROJECT_DIR)\n\nimport torch\nfrom dataclasses import dataclass\nfrom functools import partial\nimport cv2\nimport pandas as pd\nimport gc\nimport io\nimport math\nimport timm\nfrom IPython import display\nimport lpips\nfrom PIL import Image, ImageOps\nimport requests\nfrom glob import glob\nimport json\nfrom types import SimpleNamespace\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom tqdm.notebook import tqdm\nfrom CLIP import clip\nfrom resize_right import resize\nfrom guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom ipywidgets import Output\nimport hashlib\nos.chdir(f'{PROJECT_DIR}')\nfrom IPython.display import Image as ipyimg\nfrom numpy import asarray\nfrom einops import rearrange, repeat\nimport torch, torchvision\nimport time\nfrom omegaconf import OmegaConf\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# AdaBins stuff\nif USE_ADABINS:\n    try:\n        from infer import InferenceHelper\n    except:\n        if not os.path.exists(\"AdaBins\"):\n            gitclone(\"https://github.com/shariqfarooq123/AdaBins.git\")\n        if not os.path.exists(f'{PROJECT_DIR}/pretrained/AdaBins_nyu.pt'):\n            createPath(f'{PROJECT_DIR}/pretrained')\n            download_url(\"https://cloudflare-ipfs.com/ipfs/Qmd2mMnDLWePKmgfS8m6ntAg4nhV5VkUyAydYBp8cWWeB7/AdaBins_nyu.pt\", f'{PROJECT_DIR}/pretrained')\n        sys.path.append(f'{PROJECT_DIR}/AdaBins')\n    from infer import InferenceHelper\n    MAX_ADABINS_AREA = 500000\n\nimport torch\nDEVICE = torch.device('cuda:0' if (torch.cuda.is_available() and not useCPU) else 'cpu')\nprint('Using device:', DEVICE)\ndevice = DEVICE # At least one of the modules expects this name..\n\nif not useCPU:\n    if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n        print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n        torch.backends.cudnn.enabled = False","metadata":{"_uuid":"28b2937b-e9e4-46d3-ac36-553cd1d544ac","_cell_guid":"6c44b200-1b6f-4205-9acc-777a3b11e531","id":"InstallDeps","execution":{"iopub.status.busy":"2022-10-18T23:57:44.071260Z","iopub.execute_input":"2022-10-18T23:57:44.073145Z","iopub.status.idle":"2022-10-18T23:57:54.048732Z","shell.execute_reply.started":"2022-10-18T23:57:44.073108Z","shell.execute_reply":"2022-10-18T23:57:54.047223Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lpips in /opt/conda/lib/python3.7/site-packages (0.1.4)\nRequirement already satisfied: datetime in /opt/conda/lib/python3.7/site-packages (4.7)\nRequirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (0.6.11)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.7/site-packages (6.1.1)\nRequirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.7/site-packages (1.6.5)\nRequirement already satisfied: omegaconf in /opt/conda/lib/python3.7/site-packages (2.2.3)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from lpips) (1.7.3)\nRequirement already satisfied: numpy>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from lpips) (1.21.6)\nRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from lpips) (1.11.0)\nRequirement already satisfied: tqdm>=4.28.1 in /opt/conda/lib/python3.7/site-packages (from lpips) (4.64.0)\nRequirement already satisfied: torchvision>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from lpips) (0.12.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from datetime) (2022.1)\nRequirement already satisfied: zope.interface in /opt/conda/lib/python3.7/site-packages (from datetime) (5.5.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.8.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.5)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (2022.5.0)\nRequirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (3.19.4)\nRequirement already satisfied: torchmetrics>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.9.2)\nRequirement already satisfied: tensorboard>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (2.6.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (4.1.1)\nRequirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning) (0.3.2)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.7/site-packages (from omegaconf) (4.9.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.28.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.1.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.7)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.1.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.43.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.2.1->lpips) (9.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.12.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.9)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n\nUsing device: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title ### 1.4 Define Midas functions\n\nfrom midas.dpt_depth import DPTDepthModel\nfrom midas.midas_net import MidasNet\nfrom midas.midas_net_custom import MidasNet_small\nfrom midas.transforms import Resize, NormalizeImage, PrepareForNet\n\n# Initialize MiDaS depth model.\n# It remains resident in VRAM and likely takes around 2GB VRAM.\n# You could instead initialize it for each frame (and free it after each frame) to save VRAM.. but initializing it is slow.\ndefault_models = {\n    \"midas_v21_small\": f\"{model_path}/midas_v21_small-70d6b9c8.pt\",\n    \"midas_v21\": f\"{model_path}/midas_v21-f6b98070.pt\",\n    \"dpt_large\": f\"{model_path}/dpt_large-midas-2f21e586.pt\",\n    \"dpt_hybrid\": f\"{model_path}/dpt_hybrid-midas-501f0c75.pt\",\n    \"dpt_hybrid_nyu\": f\"{model_path}/dpt_hybrid_nyu-2ce69ec7.pt\",}\n\n\ndef init_midas_depth_model(midas_model_type=\"dpt_large\", optimize=True):\n    midas_model = None\n    net_w = None\n    net_h = None\n    resize_mode = None\n    normalization = None\n\n    print(f\"Initializing MiDaS '{midas_model_type}' depth model...\")\n    # load network\n    midas_model_path = default_models[midas_model_type]\n\n    if midas_model_type == \"dpt_large\": # DPT-Large\n        midas_model = DPTDepthModel(\n            path=midas_model_path,\n            backbone=\"vitl16_384\",\n            non_negative=True,\n        )\n        net_w, net_h = 384, 384\n        resize_mode = \"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    elif midas_model_type == \"dpt_hybrid\": #DPT-Hybrid\n        midas_model = DPTDepthModel(\n            path=midas_model_path,\n            backbone=\"vitb_rn50_384\",\n            non_negative=True,\n        )\n        net_w, net_h = 384, 384\n        resize_mode=\"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    elif midas_model_type == \"dpt_hybrid_nyu\": #DPT-Hybrid-NYU\n        midas_model = DPTDepthModel(\n            path=midas_model_path,\n            backbone=\"vitb_rn50_384\",\n            non_negative=True,\n        )\n        net_w, net_h = 384, 384\n        resize_mode=\"minimal\"\n        normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    elif midas_model_type == \"midas_v21\":\n        midas_model = MidasNet(midas_model_path, non_negative=True)\n        net_w, net_h = 384, 384\n        resize_mode=\"upper_bound\"\n        normalization = NormalizeImage(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        )\n    elif midas_model_type == \"midas_v21_small\":\n        midas_model = MidasNet_small(midas_model_path, features=64, backbone=\"efficientnet_lite3\", exportable=True, non_negative=True, blocks={'expand': True})\n        net_w, net_h = 256, 256\n        resize_mode=\"upper_bound\"\n        normalization = NormalizeImage(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        )\n    else:\n        print(f\"midas_model_type '{midas_model_type}' not implemented\")\n        assert False\n\n    midas_transform = T.Compose(\n        [\n            Resize(\n                net_w,\n                net_h,\n                resize_target=None,\n                keep_aspect_ratio=True,\n                ensure_multiple_of=32,\n                resize_method=resize_mode,\n                image_interpolation_method=cv2.INTER_CUBIC,\n            ),\n            normalization,\n            PrepareForNet(),\n        ]\n    )\n\n    midas_model.eval()\n    \n    if optimize==True:\n        if DEVICE == torch.device(\"cuda\"):\n            midas_model = midas_model.to(memory_format=torch.channels_last)  \n            midas_model = midas_model.half()\n\n    midas_model.to(DEVICE)\n\n    print(f\"MiDaS '{midas_model_type}' depth model initialized.\")\n    return midas_model, midas_transform, net_w, net_h, resize_mode, normalization","metadata":{"_uuid":"592c3b16-7212-43c3-b265-f21468e7113e","_cell_guid":"f30e9d18-da50-4020-901d-7c1c936da53c","id":"DefMidasFns","execution":{"iopub.status.busy":"2022-10-18T23:57:54.052375Z","iopub.execute_input":"2022-10-18T23:57:54.052754Z","iopub.status.idle":"2022-10-18T23:57:54.070492Z","shell.execute_reply.started":"2022-10-18T23:57:54.052719Z","shell.execute_reply":"2022-10-18T23:57:54.069310Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#@title 1.5 Define necessary functions\n\n# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n\nimport py3d_tools as p3dT\nimport disco_xform_utils as dxf\n\ndef interp(t):\n    return 3 * t**2 - 2 * t ** 3\n\ndef perlin(width, height, scale=10, device=None):\n    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n    wx = 1 - interp(xs)\n    wy = 1 - interp(ys)\n    dots = 0\n    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n\ndef perlin_ms(octaves, width, height, grayscale, device=device):\n    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n    for i in range(1 if grayscale else 3):\n        scale = 2 ** len(octaves)\n        oct_width = width\n        oct_height = height\n        for oct in octaves:\n            p = perlin(oct_width, oct_height, scale, device)\n            out_array[i] += p * oct\n            scale //= 2\n            oct_width *= 2\n            oct_height *= 2\n    return torch.cat(out_array)\n\ndef create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n    out = perlin_ms(octaves, width, height, grayscale)\n    if grayscale:\n        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n    else:\n        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n        out = TF.resize(size=(side_y, side_x), img=out)\n        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n\n    out = ImageOps.autocontrast(out)\n    return out\n\ndef regen_perlin():\n    if perlin_mode == 'color':\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n    elif perlin_mode == 'gray':\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n    else:\n        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n\n    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n    del init2\n    return init.expand(batch_size, -1, -1, -1)\n\ndef fetch(url_or_path):\n    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n        r = requests.get(url_or_path)\n        r.raise_for_status()\n        fd = io.BytesIO()\n        fd.write(r.content)\n        fd.seek(0)\n        return fd\n    return open(url_or_path, 'rb')\n\ndef read_image_workaround(path):\n    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n    this incompatibility to avoid colour inversions.\"\"\"\n    im_tmp = cv2.imread(path)\n    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n\ndef parse_prompt(prompt):\n    if prompt.startswith('http://') or prompt.startswith('https://'):\n        vals = prompt.rsplit(':', 2)\n        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n    else:\n        vals = prompt.rsplit(':', 1)\n    vals = vals + ['', '1'][len(vals):]\n    return vals[0], float(vals[1])\n\ndef sinc(x):\n    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n\ndef lanczos(x, a):\n    cond = torch.logical_and(-a < x, x < a)\n    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n    return out / out.sum()\n\ndef ramp(ratio, width):\n    n = math.ceil(width / ratio + 1)\n    out = torch.empty([n])\n    cur = 0\n    for i in range(out.shape[0]):\n        out[i] = cur\n        cur += ratio\n    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n\ndef resample(input, size, align_corners=True):\n    n, c, h, w = input.shape\n    dh, dw = size\n\n    input = input.reshape([n * c, 1, h, w])\n\n    if dh < h:\n        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n        pad_h = (kernel_h.shape[0] - 1) // 2\n        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n        input = F.conv2d(input, kernel_h[None, None, :, None])\n\n    if dw < w:\n        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n        pad_w = (kernel_w.shape[0] - 1) // 2\n        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n        input = F.conv2d(input, kernel_w[None, None, None, :])\n\n    input = input.reshape([n, c, h, w])\n    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n\nclass MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cutn, skip_augs=False):\n        super().__init__()\n        self.cut_size = cut_size\n        self.cutn = cutn\n        self.skip_augs = skip_augs\n        self.augs = T.Compose([\n            T.RandomHorizontalFlip(p=0.5),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            T.RandomGrayscale(p=0.15),\n            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n        ])\n\n    def forward(self, input):\n        input = T.Pad(input.shape[2]//4, fill=0)(input)\n        sideY, sideX = input.shape[2:4]\n        max_size = min(sideX, sideY)\n\n        cutouts = []\n        for ch in range(self.cutn):\n            if ch > self.cutn - self.cutn//4:\n                cutout = input.clone()\n            else:\n                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n                offsety = torch.randint(0, abs(sideY - size + 1), ())\n                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n\n            if not self.skip_augs:\n                cutout = self.augs(cutout)\n            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n            del cutout\n\n        cutouts = torch.cat(cutouts, dim=0)\n        return cutouts\n\ncutout_debug = False\npadargs = {}\n\nclass MakeCutoutsDango(nn.Module):\n    def __init__(self, cut_size,\n                 Overview=4, \n                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n                 ):\n        super().__init__()\n        self.cut_size = cut_size\n        self.Overview = Overview\n        self.InnerCrop = InnerCrop\n        self.IC_Size_Pow = IC_Size_Pow\n        self.IC_Grey_P = IC_Grey_P\n        if args.animation_mode == 'None':\n          self.augs = T.Compose([\n              T.RandomHorizontalFlip(p=0.5),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomGrayscale(p=0.1),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n          ])\n        elif args.animation_mode == 'Video Input':\n          self.augs = T.Compose([\n              T.RandomHorizontalFlip(p=0.5),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomPerspective(distortion_scale=0.4, p=0.7),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomGrayscale(p=0.15),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n          ])\n        elif  args.animation_mode == '2D' or args.animation_mode == '3D':\n          self.augs = T.Compose([\n              T.RandomHorizontalFlip(p=0.4),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomAffine(degrees=10, translate=(0.05, 0.05),  interpolation = T.InterpolationMode.BILINEAR),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.RandomGrayscale(p=0.1),\n              T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n              T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.3),\n          ])\n          \n\n    def forward(self, input):\n        cutouts = []\n        gray = T.Grayscale(3)\n        sideY, sideX = input.shape[2:4]\n        max_size = min(sideX, sideY)\n        min_size = min(sideX, sideY, self.cut_size)\n        l_size = max(sideX, sideY)\n        output_shape = [1,3,self.cut_size,self.cut_size] \n        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n        pad_input = F.pad(input,((sideY-max_size)//2,(sideY-max_size)//2,(sideX-max_size)//2,(sideX-max_size)//2), **padargs)\n        cutout = resize(pad_input, out_shape=output_shape)\n\n        if self.Overview>0:\n            if self.Overview<=4:\n                if self.Overview>=1:\n                    cutouts.append(cutout)\n                if self.Overview>=2:\n                    cutouts.append(gray(cutout))\n                if self.Overview>=3:\n                    cutouts.append(TF.hflip(cutout))\n                if self.Overview==4:\n                    cutouts.append(gray(TF.hflip(cutout)))\n            else:\n                cutout = resize(pad_input, out_shape=output_shape)\n                for _ in range(self.Overview):\n                    cutouts.append(cutout)\n\n            if cutout_debug:\n                TF.to_pil_image(cutouts[0].clamp(0, 1).squeeze(0)).save(\"cutout_overview0.jpg\",quality=99)\n\n                              \n        if self.InnerCrop >0:\n            for i in range(self.InnerCrop):\n                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n                offsetx = torch.randint(0, sideX - size + 1, ())\n                offsety = torch.randint(0, sideY - size + 1, ())\n                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n                if i <= int(self.IC_Grey_P * self.InnerCrop):\n                    cutout = gray(cutout)\n                cutout = resize(cutout, out_shape=output_shape)\n                cutouts.append(cutout)\n            if cutout_debug:\n                TF.to_pil_image(cutouts[-1].clamp(0, 1).squeeze(0)).save(\"cutout_InnerCrop.jpg\",quality=99)\n        cutouts = torch.cat(cutouts)\n        if skip_augs is not True: cutouts=self.augs(cutouts)\n        return cutouts\n\ndef spherical_dist_loss(x, y):\n    x = F.normalize(x, dim=-1)\n    y = F.normalize(y, dim=-1)\n    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)     \n\ndef tv_loss(input):\n    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n\n\ndef range_loss(input):\n    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n\nstop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\nTRANSLATION_SCALE = 1.0/200.0\n\ndef do_3d_step(img_filepath, frame_num, midas_model, midas_transform):\n  if args.key_frames:\n    translation_x = args.translation_x_series[frame_num]\n    translation_y = args.translation_y_series[frame_num]\n    translation_z = args.translation_z_series[frame_num]\n    rotation_3d_x = args.rotation_3d_x_series[frame_num]\n    rotation_3d_y = args.rotation_3d_y_series[frame_num]\n    rotation_3d_z = args.rotation_3d_z_series[frame_num]\n    print(\n        f'translation_x: {translation_x}',\n        f'translation_y: {translation_y}',\n        f'translation_z: {translation_z}',\n        f'rotation_3d_x: {rotation_3d_x}',\n        f'rotation_3d_y: {rotation_3d_y}',\n        f'rotation_3d_z: {rotation_3d_z}',\n    )\n\n  translate_xyz = [-translation_x*TRANSLATION_SCALE, translation_y*TRANSLATION_SCALE, -translation_z*TRANSLATION_SCALE]\n  rotate_xyz_degrees = [rotation_3d_x, rotation_3d_y, rotation_3d_z]\n  print('translation:',translate_xyz)\n  print('rotation:',rotate_xyz_degrees)\n  rotate_xyz = [math.radians(rotate_xyz_degrees[0]), math.radians(rotate_xyz_degrees[1]), math.radians(rotate_xyz_degrees[2])]\n  rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n  print(\"rot_mat: \" + str(rot_mat))\n  next_step_pil = dxf.transform_image_3d(img_filepath, midas_model, midas_transform, DEVICE,\n                                          rot_mat, translate_xyz, args.near_plane, args.far_plane,\n                                          args.fov, padding_mode=args.padding_mode,\n                                          sampling_mode=args.sampling_mode, midas_weight=args.midas_weight)\n  return next_step_pil\n\ndef symmetry_transformation_fn(x):\n    if args.use_horizontal_symmetry:\n        [n, c, h, w] = x.size()\n        x = torch.concat((x[:, :, :, :w//2], torch.flip(x[:, :, :, :w//2], [-1])), -1)\n        print(\"horizontal symmetry applied\")\n    if args.use_vertical_symmetry:\n        [n, c, h, w] = x.size()\n        x = torch.concat((x[:, :, :h//2, :], torch.flip(x[:, :, :h//2, :], [-2])), -2)\n        print(\"vertical symmetry applied\")\n    return x\n\ndef do_run():\n  seed = args.seed\n  print(range(args.start_frame, args.max_frames))\n\n  if (args.animation_mode == \"3D\") and (args.midas_weight > 0.0):\n      midas_model, midas_transform, midas_net_w, midas_net_h, midas_resize_mode, midas_normalization = init_midas_depth_model(args.midas_depth_model)\n  for frame_num in range(args.start_frame, args.max_frames):\n      if stop_on_next_loop:\n        break\n      \n      display.clear_output(wait=True)\n\n      # Print Frame progress if animation mode is on\n      if args.animation_mode != \"None\":\n        batchBar = tqdm(range(args.max_frames), desc =\"Frames\")\n        batchBar.n = frame_num\n        batchBar.refresh()\n\n      \n      # Inits if not video frames\n      if args.animation_mode != \"Video Input\":\n        if args.init_image in ['','none', 'None', 'NONE']:\n          init_image = None\n        else:\n          init_image = args.init_image\n        init_scale = args.init_scale\n        skip_steps = args.skip_steps\n\n      if args.animation_mode == \"2D\":\n        if args.key_frames:\n          angle = args.angle_series[frame_num]\n          zoom = args.zoom_series[frame_num]\n          translation_x = args.translation_x_series[frame_num]\n          translation_y = args.translation_y_series[frame_num]\n          print(\n              f'angle: {angle}',\n              f'zoom: {zoom}',\n              f'translation_x: {translation_x}',\n              f'translation_y: {translation_y}',\n          )\n        \n        if frame_num > 0:\n          seed += 1\n          if resume_run and frame_num == start_frame:\n            img_0 = cv2.imread(batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\")\n          else:\n            img_0 = cv2.imread('prevFrame.png')\n          center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n          trans_mat = np.float32(\n              [[1, 0, translation_x],\n              [0, 1, translation_y]]\n          )\n          rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n          trans_mat = np.vstack([trans_mat, [0,0,1]])\n          rot_mat = np.vstack([rot_mat, [0,0,1]])\n          transformation_matrix = np.matmul(rot_mat, trans_mat)\n          img_0 = cv2.warpPerspective(\n              img_0,\n              transformation_matrix,\n              (img_0.shape[1], img_0.shape[0]),\n              borderMode=cv2.BORDER_WRAP\n          )\n\n          cv2.imwrite('prevFrameScaled.png', img_0)\n          init_image = 'prevFrameScaled.png'\n          init_scale = args.frames_scale\n          skip_steps = args.calc_frames_skip_steps\n\n      if args.animation_mode == \"3D\":\n        if frame_num > 0:\n          seed += 1    \n          if resume_run and frame_num == start_frame:\n            img_filepath = batchFolder+f\"/{batch_name}({batchNum})_{start_frame-1:04}.png\"\n            if turbo_mode and frame_num > turbo_preroll:\n              shutil.copyfile(img_filepath, 'oldFrameScaled.png')\n          else:\n            img_filepath = 'prevFrame.png'\n\n          next_step_pil = do_3d_step(img_filepath, frame_num, midas_model, midas_transform)\n          next_step_pil.save('prevFrameScaled.png')\n\n          ### Turbo mode - skip some diffusions, use 3d morph for clarity and to save time\n          if turbo_mode:\n            if frame_num == turbo_preroll: #start tracking oldframe\n              next_step_pil.save('oldFrameScaled.png')#stash for later blending          \n            elif frame_num > turbo_preroll:\n              #set up 2 warped image sequences, old & new, to blend toward new diff image\n              old_frame = do_3d_step('oldFrameScaled.png', frame_num, midas_model, midas_transform)\n              old_frame.save('oldFrameScaled.png')\n              if frame_num % int(turbo_steps) != 0: \n                print('turbo skip this frame: skipping clip diffusion steps')\n                filename = f'{args.batch_name}({args.batchNum})_{frame_num:04}.png'\n                blend_factor = ((frame_num % int(turbo_steps))+1)/int(turbo_steps)\n                print('turbo skip this frame: skipping clip diffusion steps and saving blended frame')\n                newWarpedImg = cv2.imread('prevFrameScaled.png')#this is already updated..\n                oldWarpedImg = cv2.imread('oldFrameScaled.png')\n                blendedImage = cv2.addWeighted(newWarpedImg, blend_factor, oldWarpedImg,1-blend_factor, 0.0)\n                cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n                next_step_pil.save(f'{img_filepath}') # save it also as prev_frame to feed next iteration\n                if vr_mode:\n                  generate_eye_views(TRANSLATION_SCALE,batchFolder,filename,frame_num,midas_model, midas_transform)\n                continue\n              else:\n                #if not a skip frame, will run diffusion and need to blend.\n                oldWarpedImg = cv2.imread('prevFrameScaled.png')\n                cv2.imwrite(f'oldFrameScaled.png',oldWarpedImg)#swap in for blending later \n                print('clip/diff this frame - generate clip diff image')\n\n          init_image = 'prevFrameScaled.png'\n          init_scale = args.frames_scale\n          skip_steps = args.calc_frames_skip_steps\n\n      if  args.animation_mode == \"Video Input\":\n        init_scale = args.video_init_frames_scale\n        skip_steps = args.calc_frames_skip_steps\n        if not video_init_seed_continuity:\n          seed += 1\n        if video_init_flow_warp:\n          if frame_num == 0: \n            skip_steps = args.video_init_skip_steps\n            init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n          if frame_num > 0: \n            prev = PIL.Image.open(batchFolder+f\"/{batch_name}({batchNum})_{frame_num-1:04}.png\")\n            \n            frame1_path = f'{videoFramesFolder}/{frame_num:04}.jpg'\n            frame2 = PIL.Image.open(f'{videoFramesFolder}/{frame_num+1:04}.jpg')\n            flo_path = f\"/{flo_folder}/{frame1_path.split('/')[-1]}.npy\"\n            \n            init_image = 'warped.png'\n            print(video_init_flow_blend)\n            weights_path = None\n            if video_init_check_consistency:\n                # TBD\n                pass\n \n            warp(prev, frame2, flo_path, blend=video_init_flow_blend, weights_path=weights_path).save(init_image)\n            \n        else:\n          init_image = f'{videoFramesFolder}/{frame_num+1:04}.jpg'\n\n\n      loss_values = []\n  \n      if seed is not None:\n          np.random.seed(seed)\n          random.seed(seed)\n          torch.manual_seed(seed)\n          torch.cuda.manual_seed_all(seed)\n          torch.backends.cudnn.deterministic = True\n  \n      target_embeds, weights = [], []\n      \n      if args.prompts_series is not None and frame_num >= len(args.prompts_series):\n        frame_prompt = args.prompts_series[-1]\n      elif args.prompts_series is not None:\n        frame_prompt = args.prompts_series[frame_num]\n      else:\n        frame_prompt = []\n      \n      print(args.image_prompts_series)\n      if args.image_prompts_series is not None and frame_num >= len(args.image_prompts_series):\n        image_prompt = args.image_prompts_series[-1]\n      elif args.image_prompts_series is not None:\n        image_prompt = args.image_prompts_series[frame_num]\n      else:\n        image_prompt = []\n\n      print(f'Frame {frame_num} Prompt: {frame_prompt}')\n\n      model_stats = []\n      for clip_model in clip_models:\n            cutn = 16\n            model_stat = {\"clip_model\":None,\"target_embeds\":[],\"make_cutouts\":None,\"weights\":[]}\n            model_stat[\"clip_model\"] = clip_model\n            \n            for prompt in frame_prompt:\n                txt, weight = parse_prompt(prompt)\n                txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n                \n                if args.fuzzy_prompt:\n                    for i in range(25):\n                        model_stat[\"target_embeds\"].append((txt + torch.randn(txt.shape).cuda() * args.rand_mag).clamp(0,1))\n                        model_stat[\"weights\"].append(weight)\n                else:\n                    model_stat[\"target_embeds\"].append(txt)\n                    model_stat[\"weights\"].append(weight)\n        \n            if image_prompt:\n              model_stat[\"make_cutouts\"] = MakeCutouts(clip_model.visual.input_resolution, cutn, skip_augs=skip_augs) \n              for prompt in image_prompt:\n                  path, weight = parse_prompt(prompt)\n                  img = Image.open(fetch(path)).convert('RGB')\n                  img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n                  batch = model_stat[\"make_cutouts\"](TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n                  embed = clip_model.encode_image(normalize(batch)).float()\n                  if fuzzy_prompt:\n                      for i in range(25):\n                          model_stat[\"target_embeds\"].append((embed + torch.randn(embed.shape).cuda() * rand_mag).clamp(0,1))\n                          weights.extend([weight / cutn] * cutn)\n                  else:\n                      model_stat[\"target_embeds\"].append(embed)\n                      model_stat[\"weights\"].extend([weight / cutn] * cutn)\n        \n            model_stat[\"target_embeds\"] = torch.cat(model_stat[\"target_embeds\"])\n            model_stat[\"weights\"] = torch.tensor(model_stat[\"weights\"], device=device)\n            if model_stat[\"weights\"].sum().abs() < 1e-3:\n                raise RuntimeError('The weights must not sum to 0.')\n            model_stat[\"weights\"] /= model_stat[\"weights\"].sum().abs()\n            model_stats.append(model_stat)\n  \n      init = None\n      if init_image is not None:\n          init = Image.open(fetch(init_image)).convert('RGB')\n          init = init.resize((args.side_x, args.side_y), Image.LANCZOS)\n          init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n      \n      if args.perlin_init:\n          if args.perlin_mode == 'color':\n              init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n              init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n          elif args.perlin_mode == 'gray':\n            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n          else:\n            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n          # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n          init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n          del init2\n  \n      cur_t = None\n  \n      def cond_fn(x, t, y=None):\n          with torch.enable_grad():\n              x_is_NaN = False\n              x = x.detach().requires_grad_()\n              n = x.shape[0]\n              if use_secondary_model is True:\n                alpha = torch.tensor(diffusion.sqrt_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n                sigma = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[cur_t], device=device, dtype=torch.float32)\n                cosine_t = alpha_sigma_to_t(alpha, sigma)\n                out = secondary_model(x, cosine_t[None].repeat([n])).pred\n                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n                x_in = out * fac + x * (1 - fac)\n                x_in_grad = torch.zeros_like(x_in)\n              else:\n                my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n                out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n                fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n                x_in = out['pred_xstart'] * fac + x * (1 - fac)\n                x_in_grad = torch.zeros_like(x_in)\n              for model_stat in model_stats:\n                for i in range(args.cutn_batches):\n                    t_int = int(t.item())+1 #errors on last step without +1, need to find source\n                    #when using SLIP Base model the dimensions need to be hard coded to avoid AttributeError: 'VisionTransformer' object has no attribute 'input_resolution'\n                    try:\n                        input_resolution=model_stat[\"clip_model\"].visual.input_resolution\n                    except:\n                        input_resolution=224\n                    # liminal specific condition\n                    if diffusion_model in liminal_model_names: \n                        cuts = MakeCutoutsDango(input_resolution,\n                        Overview= args.cut_overview[1000-t_int], \n                        InnerCrop = args.cut_innercut[1000-t_int],\n                        IC_Size_Pow = args.cut_ic_pow,\n                        IC_Grey_P = args.cut_icgray_p[1000-t_int]\n                        )\n                    else: #default cuts\n                        cuts = MakeCutoutsDango(input_resolution,\n                        Overview = args.cut_overview[1000-t_int], \n                        InnerCrop = args.cut_innercut[1000-t_int],\n                        IC_Size_Pow = args.cut_ic_pow[1000-t_int],\n                        IC_Grey_P = args.cut_icgray_p[1000-t_int]\n                        )\n                    clip_in = normalize(cuts(x_in.add(1).div(2)))\n                    image_embeds = model_stat[\"clip_model\"].encode_image(clip_in).float()\n                    dists = spherical_dist_loss(image_embeds.unsqueeze(1), model_stat[\"target_embeds\"].unsqueeze(0))\n                    dists = dists.view([args.cut_overview[1000-t_int]+args.cut_innercut[1000-t_int], n, -1])\n                    losses = dists.mul(model_stat[\"weights\"]).sum(2).mean(0)\n                    loss_values.append(losses.sum().item()) # log loss, probably shouldn't do per cutn_batch\n                    x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n              tv_losses = tv_loss(x_in)\n              if use_secondary_model is True:\n                range_losses = range_loss(out)\n              else:\n                range_losses = range_loss(out['pred_xstart'])\n              sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n              loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n              if init is not None and init_scale:\n                  init_losses = lpips_model(x_in, init)\n                  loss = loss + init_losses.sum() * init_scale\n              x_in_grad += torch.autograd.grad(loss, x_in)[0]\n              if torch.isnan(x_in_grad).any()==False:\n                  grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n              else:\n                # print(\"NaN'd\")\n                x_is_NaN = True\n                grad = torch.zeros_like(x)\n          if args.clamp_grad and x_is_NaN == False:\n              magnitude = grad.square().mean().sqrt()\n              return grad * magnitude.clamp(max=args.clamp_max) / magnitude  #min=-0.02, min=-clamp_max, \n          return grad\n  \n      if args.diffusion_sampling_mode == 'ddim':\n          sample_fn = diffusion.ddim_sample_loop_progressive\n      else:\n          sample_fn = diffusion.plms_sample_loop_progressive\n\n\n      image_display = Output()\n      for i in range(args.n_batches):\n          if args.animation_mode == 'None':\n            display.clear_output(wait=True)\n            batchBar = tqdm(range(args.n_batches), desc =\"Batches\")\n            batchBar.n = i\n            batchBar.refresh()\n          print('')\n          display.display(image_display)\n          gc.collect()\n          torch.cuda.empty_cache()\n          cur_t = diffusion.num_timesteps - skip_steps - 1\n          total_steps = cur_t\n\n          if perlin_init:\n              init = regen_perlin()\n\n          if args.diffusion_sampling_mode == 'ddim':\n              samples = sample_fn(\n                  model,\n                  (batch_size, 3, args.side_y, args.side_x),\n                  clip_denoised=clip_denoised,\n                  model_kwargs={},\n                  cond_fn=cond_fn,\n                  progress=True,\n                  skip_timesteps=skip_steps,\n                  init_image=init,\n                  randomize_class=randomize_class,\n                  eta=eta,\n                  transformation_fn=symmetry_transformation_fn,\n                  transformation_percent=args.transformation_percent\n              )\n          else:\n              samples = sample_fn(\n                  model,\n                  (batch_size, 3, args.side_y, args.side_x),\n                  clip_denoised=clip_denoised,\n                  model_kwargs={},\n                  cond_fn=cond_fn,\n                  progress=True,\n                  skip_timesteps=skip_steps,\n                  init_image=init,\n                  randomize_class=randomize_class,\n                  order=2,\n              )\n          \n          \n          # with run_display:\n          # display.clear_output(wait=True)\n          for j, sample in enumerate(samples):    \n            cur_t -= 1\n            intermediateStep = False\n            if args.steps_per_checkpoint is not None:\n                if j % steps_per_checkpoint == 0 and j > 0:\n                  intermediateStep = True\n            elif j in args.intermediate_saves:\n              intermediateStep = True\n            with image_display:\n              if j % args.display_rate == 0 or cur_t == -1 or intermediateStep == True:\n                  for k, image in enumerate(sample['pred_xstart']):\n                      # tqdm.write(f'Batch {i}, step {j}, output {k}:')\n                      current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n                      percent = math.ceil(j/total_steps*100)\n                      if args.n_batches > 0:\n                        #if intermediates are saved to the subfolder, don't append a step or percentage to the name\n                        if cur_t == -1 and args.intermediates_in_subfolder is True:\n                          save_num = f'{frame_num:04}' if animation_mode != \"None\" else i\n                          filename = f'{args.batch_name}({args.batchNum})_{save_num}.png'\n                        else:\n                          #If we're working with percentages, append it\n                          if args.steps_per_checkpoint is not None:\n                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{percent:02}%.png'\n                          # Or else, iIf we're working with specific steps, append those\n                          else:\n                            filename = f'{args.batch_name}({args.batchNum})_{i:04}-{j:03}.png'\n                      image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n                      if j % args.display_rate == 0 or cur_t == -1:\n                        image.save('progress.png')\n                        display.clear_output(wait=True)\n                        display.display(display.Image('progress.png'))\n                      if args.steps_per_checkpoint is not None:\n                        if j % args.steps_per_checkpoint == 0 and j > 0:\n                          if args.intermediates_in_subfolder is True:\n                            image.save(f'{partialFolder}/{filename}')\n                          else:\n                            image.save(f'{batchFolder}/{filename}')\n                      else:\n                        if j in args.intermediate_saves:\n                          if args.intermediates_in_subfolder is True:\n                            image.save(f'{partialFolder}/{filename}')\n                          else:\n                            image.save(f'{batchFolder}/{filename}')\n                      if cur_t == -1:\n                        if frame_num == 0:\n                          save_settings()\n                        if args.animation_mode != \"None\":\n                          image.save('prevFrame.png')\n                        image.save(f'{batchFolder}/{filename}')\n                        if args.animation_mode == \"3D\":\n                          # If turbo, save a blended image\n                          if turbo_mode and frame_num > 0:\n                            # Mix new image with prevFrameScaled\n                            blend_factor = (1)/int(turbo_steps)\n                            newFrame = cv2.imread('prevFrame.png') # This is already updated..\n                            prev_frame_warped = cv2.imread('prevFrameScaled.png')\n                            blendedImage = cv2.addWeighted(newFrame, blend_factor, prev_frame_warped, (1-blend_factor), 0.0)\n                            cv2.imwrite(f'{batchFolder}/{filename}',blendedImage)\n                          else:\n                            image.save(f'{batchFolder}/{filename}')\n\n                          if vr_mode:\n                            generate_eye_views(TRANSLATION_SCALE, batchFolder, filename, frame_num, midas_model, midas_transform)\n\n                        # if frame_num != args.max_frames-1:\n                        #   display.clear_output()\n          \n          plt.plot(np.array(loss_values), 'r')\n\ndef generate_eye_views(trans_scale,batchFolder,filename,frame_num,midas_model, midas_transform):\n   for i in range(2):\n      theta = vr_eye_angle * (math.pi/180)\n      ray_origin = math.cos(theta) * vr_ipd / 2 * (-1.0 if i==0 else 1.0)\n      ray_rotation = (theta if i==0 else -theta)\n      translate_xyz = [-(ray_origin)*trans_scale, 0,0]\n      rotate_xyz = [0, (ray_rotation), 0]\n      rot_mat = p3dT.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n      transformed_image = dxf.transform_image_3d(f'{batchFolder}/{filename}', midas_model, midas_transform, DEVICE,\n                                                      rot_mat, translate_xyz, args.near_plane, args.far_plane,\n                                                      args.fov, padding_mode=args.padding_mode,\n                                                      sampling_mode=args.sampling_mode, midas_weight=args.midas_weight,spherical=True)\n      eye_file_path = batchFolder+f\"/frame_{frame_num:04}\" + ('_l' if i==0 else '_r')+'.png'\n      transformed_image.save(eye_file_path)\n\ndef save_settings():\n    setting_list = {\n      'text_prompts': text_prompts,\n      'image_prompts': image_prompts,\n      'clip_guidance_scale': clip_guidance_scale,\n      'tv_scale': tv_scale,\n      'range_scale': range_scale,\n      'sat_scale': sat_scale,\n      # 'cutn': cutn,\n      'cutn_batches': cutn_batches,\n      'max_frames': max_frames,\n      'interp_spline': interp_spline,\n      # 'rotation_per_frame': rotation_per_frame,\n      'init_image': init_image,\n      'init_scale': init_scale,\n      'skip_steps': skip_steps,\n      # 'zoom_per_frame': zoom_per_frame,\n      'frames_scale': frames_scale,\n      'frames_skip_steps': frames_skip_steps,\n      'perlin_init': perlin_init,\n      'perlin_mode': perlin_mode,\n      'skip_augs': skip_augs,\n      'randomize_class': randomize_class,\n      'clip_denoised': clip_denoised,\n      'clamp_grad': clamp_grad,\n      'clamp_max': clamp_max,\n      'seed': seed,\n      'fuzzy_prompt': fuzzy_prompt,\n      'rand_mag': rand_mag,\n      'eta': eta,\n      'width': width_height[0],\n      'height': width_height[1],\n      'diffusion_model': diffusion_model,\n      'use_secondary_model': use_secondary_model,\n      'steps': steps,\n      'diffusion_steps': diffusion_steps,\n      'diffusion_sampling_mode': diffusion_sampling_mode,\n      'ViTB32': ViTB32,\n      'ViTB16': ViTB16,\n      'ViTL14': ViTL14,\n      'ViTL14_336px': ViTL14_336px,\n      'RN101': RN101,\n      'RN50': RN50,\n      'RN50x4': RN50x4,\n      'RN50x16': RN50x16,\n      'RN50x64': RN50x64,\n      'ViTB32_laion2b_e16': ViTB32_laion2b_e16,\n      'ViTB32_laion400m_e31': ViTB32_laion400m_e31,\n      'ViTB32_laion400m_32': ViTB32_laion400m_32,\n      'ViTB32quickgelu_laion400m_e31': ViTB32quickgelu_laion400m_e31,\n      'ViTB32quickgelu_laion400m_e32': ViTB32quickgelu_laion400m_e32,\n      'ViTB16_laion400m_e31': ViTB16_laion400m_e31,\n      'ViTB16_laion400m_e32': ViTB16_laion400m_e32,\n      'RN50_yffcc15m': RN50_yffcc15m,\n      'RN50_cc12m': RN50_cc12m,\n      'RN50_quickgelu_yfcc15m': RN50_quickgelu_yfcc15m,\n      'RN50_quickgelu_cc12m': RN50_quickgelu_cc12m,\n      'RN101_yfcc15m': RN101_yfcc15m,\n      'RN101_quickgelu_yfcc15m': RN101_quickgelu_yfcc15m,\n      'cut_overview': str(cut_overview),\n      'cut_innercut': str(cut_innercut),\n      'cut_ic_pow': str(cut_ic_pow),\n      'cut_icgray_p': str(cut_icgray_p),\n      'key_frames': key_frames,\n      'max_frames': max_frames,\n      'angle': angle,\n      'zoom': zoom,\n      'translation_x': translation_x,\n      'translation_y': translation_y,\n      'translation_z': translation_z,\n      'rotation_3d_x': rotation_3d_x,\n      'rotation_3d_y': rotation_3d_y,\n      'rotation_3d_z': rotation_3d_z,\n      'midas_depth_model': midas_depth_model,\n      'midas_weight': midas_weight,\n      'near_plane': near_plane,\n      'far_plane': far_plane,\n      'fov': fov,\n      'padding_mode': padding_mode,\n      'sampling_mode': sampling_mode,\n      'video_init_path':video_init_path,\n      'extract_nth_frame':extract_nth_frame,\n      'video_init_seed_continuity': video_init_seed_continuity,\n      'turbo_mode':turbo_mode,\n      'turbo_steps':turbo_steps,\n      'turbo_preroll':turbo_preroll,\n      'use_horizontal_symmetry':use_horizontal_symmetry,\n      'use_vertical_symmetry':use_vertical_symmetry,\n      'transformation_percent':transformation_percent,\n      #video init settings\n      'video_init_steps': video_init_steps,\n      'video_init_clip_guidance_scale': video_init_clip_guidance_scale,\n      'video_init_tv_scale': video_init_tv_scale,\n      'video_init_range_scale': video_init_range_scale,\n      'video_init_sat_scale': video_init_sat_scale,\n      'video_init_cutn_batches': video_init_cutn_batches,\n      'video_init_skip_steps': video_init_skip_steps,\n      'video_init_frames_scale': video_init_frames_scale,\n      'video_init_frames_skip_steps': video_init_frames_skip_steps,\n      #warp settings\n      'video_init_flow_warp':video_init_flow_warp,\n      'video_init_flow_blend':video_init_flow_blend,\n      'video_init_check_consistency':video_init_check_consistency,\n      'video_init_blend_mode':video_init_blend_mode\n    }\n    # print('Settings:', setting_list)\n    with open(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\", \"w+\") as f:   #save settings\n        json.dump(setting_list, f, ensure_ascii=False, indent=4)","metadata":{"_uuid":"24eb1dc5-b1b7-40ab-a495-29dce46c201f","_cell_guid":"a93b429d-0be5-4ab4-b8bb-4cb632d84c11","id":"DefFns","execution":{"iopub.status.busy":"2022-10-18T23:57:54.074324Z","iopub.execute_input":"2022-10-18T23:57:54.074705Z","iopub.status.idle":"2022-10-18T23:57:54.228031Z","shell.execute_reply.started":"2022-10-18T23:57:54.074669Z","shell.execute_reply":"2022-10-18T23:57:54.227128Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#@title 1.6 Define the secondary diffusion model\n\ndef append_dims(x, n):\n    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n\n\ndef expand_to_planes(x, shape):\n    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n\n\ndef alpha_sigma_to_t(alpha, sigma):\n    return torch.atan2(sigma, alpha) * 2 / math.pi\n\n\ndef t_to_alpha_sigma(t):\n    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n\n\n@dataclass\nclass DiffusionOutput:\n    v: torch.Tensor\n    pred: torch.Tensor\n    eps: torch.Tensor\n\n\nclass ConvBlock(nn.Sequential):\n    def __init__(self, c_in, c_out):\n        super().__init__(\n            nn.Conv2d(c_in, c_out, 3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n\n\nclass SkipBlock(nn.Module):\n    def __init__(self, main, skip=None):\n        super().__init__()\n        self.main = nn.Sequential(*main)\n        self.skip = skip if skip else nn.Identity()\n\n    def forward(self, input):\n        return torch.cat([self.main(input), self.skip(input)], dim=1)\n\n\nclass FourierFeatures(nn.Module):\n    def __init__(self, in_features, out_features, std=1.):\n        super().__init__()\n        assert out_features % 2 == 0\n        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n\n    def forward(self, input):\n        f = 2 * math.pi * input @ self.weight.T\n        return torch.cat([f.cos(), f.sin()], dim=-1)\n\n\nclass SecondaryDiffusionImageNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        c = 64  # The base channel count\n\n        self.timestep_embed = FourierFeatures(1, 16)\n\n        self.net = nn.Sequential(\n            ConvBlock(3 + 16, c),\n            ConvBlock(c, c),\n            SkipBlock([\n                nn.AvgPool2d(2),\n                ConvBlock(c, c * 2),\n                ConvBlock(c * 2, c * 2),\n                SkipBlock([\n                    nn.AvgPool2d(2),\n                    ConvBlock(c * 2, c * 4),\n                    ConvBlock(c * 4, c * 4),\n                    SkipBlock([\n                        nn.AvgPool2d(2),\n                        ConvBlock(c * 4, c * 8),\n                        ConvBlock(c * 8, c * 4),\n                        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                    ]),\n                    ConvBlock(c * 8, c * 4),\n                    ConvBlock(c * 4, c * 2),\n                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                ]),\n                ConvBlock(c * 4, c * 2),\n                ConvBlock(c * 2, c),\n                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            ]),\n            ConvBlock(c * 2, c),\n            nn.Conv2d(c, 3, 3, padding=1),\n        )\n\n    def forward(self, input, t):\n        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n        v = self.net(torch.cat([input, timestep_embed], dim=1))\n        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n        pred = input * alphas - v * sigmas\n        eps = input * sigmas + v * alphas\n        return DiffusionOutput(v, pred, eps)\n\n\nclass SecondaryDiffusionImageNet2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        c = 64  # The base channel count\n        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n\n        self.timestep_embed = FourierFeatures(1, 16)\n        self.down = nn.AvgPool2d(2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n\n        self.net = nn.Sequential(\n            ConvBlock(3 + 16, cs[0]),\n            ConvBlock(cs[0], cs[0]),\n            SkipBlock([\n                self.down,\n                ConvBlock(cs[0], cs[1]),\n                ConvBlock(cs[1], cs[1]),\n                SkipBlock([\n                    self.down,\n                    ConvBlock(cs[1], cs[2]),\n                    ConvBlock(cs[2], cs[2]),\n                    SkipBlock([\n                        self.down,\n                        ConvBlock(cs[2], cs[3]),\n                        ConvBlock(cs[3], cs[3]),\n                        SkipBlock([\n                            self.down,\n                            ConvBlock(cs[3], cs[4]),\n                            ConvBlock(cs[4], cs[4]),\n                            SkipBlock([\n                                self.down,\n                                ConvBlock(cs[4], cs[5]),\n                                ConvBlock(cs[5], cs[5]),\n                                ConvBlock(cs[5], cs[5]),\n                                ConvBlock(cs[5], cs[4]),\n                                self.up,\n                            ]),\n                            ConvBlock(cs[4] * 2, cs[4]),\n                            ConvBlock(cs[4], cs[3]),\n                            self.up,\n                        ]),\n                        ConvBlock(cs[3] * 2, cs[3]),\n                        ConvBlock(cs[3], cs[2]),\n                        self.up,\n                    ]),\n                    ConvBlock(cs[2] * 2, cs[2]),\n                    ConvBlock(cs[2], cs[1]),\n                    self.up,\n                ]),\n                ConvBlock(cs[1] * 2, cs[1]),\n                ConvBlock(cs[1], cs[0]),\n                self.up,\n            ]),\n            ConvBlock(cs[0] * 2, cs[0]),\n            nn.Conv2d(cs[0], 3, 3, padding=1),\n        )\n\n    def forward(self, input, t):\n        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n        v = self.net(torch.cat([input, timestep_embed], dim=1))\n        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n        pred = input * alphas - v * sigmas\n        eps = input * sigmas + v * alphas\n        return DiffusionOutput(v, pred, eps)","metadata":{"_uuid":"7fe2b575-2ffa-481c-a239-3b7305a28e92","_cell_guid":"438b7214-b656-44ef-a059-85a9f158507b","id":"DefSecModel","execution":{"iopub.status.busy":"2022-10-18T23:57:54.229598Z","iopub.execute_input":"2022-10-18T23:57:54.229954Z","iopub.status.idle":"2022-10-18T23:57:54.259873Z","shell.execute_reply.started":"2022-10-18T23:57:54.229919Z","shell.execute_reply":"2022-10-18T23:57:54.259036Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# 2. Diffusion and CLIP model settings","metadata":{"_uuid":"eb026f15-e9c4-40cb-87b4-7a8e434c06fd","_cell_guid":"3a2ed437-1963-491f-b582-1406a6a15ec0","id":"DiffClipSetTop","trusted":true}},{"cell_type":"code","source":"#@markdown ####**Models Settings (note: For pixel art, the best is pixelartdiffusion_expanded):**\ndiffusion_model = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\", \"portrait_generator_v001\", \"portrait_generator_v1.5\", \"pixelartdiffusion_expanded\", \"pixel_art_diffusion_hard_256\", \"pixel_art_diffusion_soft_256\", \"pixelartdiffusion4k\", \"watercolordiffusion_2\", \"watercolordiffusion\", \"PulpSciFiDiffusion\", \"lithographydiffusion\", \"medievaldiffusion\", \"liminal_diffusion_v1\", \"liminal_diffusion_source\", \"floral_diffusion\", \"FeiArt_Handpainted_CG_Diffusion\", \"textilediffusion\", \"IsometricDiffusionRevrart512px\", \"custom\"]\n\nuse_secondary_model = False #@param {type: 'boolean'}\ndiffusion_sampling_mode = 'ddim' #@param ['plms','ddim']\n#@markdown #####**Custom model:**\ncustom_path = '/content/drive/MyDrive/AI/Disco_Diffusion/models/ema_0.9999_165000.pt'#@param {type: 'string'}\n\n#@markdown #####**CLIP settings:**\nuse_checkpoint = True #@param {type: 'boolean'}\nViTB32 = True #@param{type:\"boolean\"}\nViTB16 = True #@param{type:\"boolean\"}\nViTL14 = True #@param{type:\"boolean\"}\nViTL14_336px = False #@param{type:\"boolean\"}\nRN101 = True #@param{type:\"boolean\"}\nRN50 = False #@param{type:\"boolean\"}\nRN50x4 = False #@param{type:\"boolean\"}\nRN50x16 = False #@param{type:\"boolean\"}\nRN50x64 = False #@param{type:\"boolean\"}\n\n#@markdown #####**OpenCLIP settings:**\nViTB32_laion2b_e16 = False #@param{type:\"boolean\"}\nViTB32_laion400m_e31 = False #@param{type:\"boolean\"}\nViTB32_laion400m_32 = False #@param{type:\"boolean\"}\nViTB32quickgelu_laion400m_e31 = False #@param{type:\"boolean\"}\nViTB32quickgelu_laion400m_e32 = False #@param{type:\"boolean\"}\nViTB16_laion400m_e31 = False #@param{type:\"boolean\"}\nViTB16_laion400m_e32 = False #@param{type:\"boolean\"}\nRN50_yffcc15m = False #@param{type:\"boolean\"}\nRN50_cc12m = False #@param{type:\"boolean\"}\nRN50_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\nRN50_quickgelu_cc12m = False #@param{type:\"boolean\"}\nRN101_yfcc15m = False #@param{type:\"boolean\"}\nRN101_quickgelu_yfcc15m = False #@param{type:\"boolean\"}\n\n#@markdown If you're having issues with model downloads, check this to compare SHA's:\ncheck_model_SHA = False #@param{type:\"boolean\"}\n\ndiff_model_map = {\n    '256x256_diffusion_uncond': { 'downloaded': False, 'sha': 'a37c32fffd316cd494cf3f35b339936debdc1576dad13fe57c42399a5dbc78b1', 'uri_list': ['https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt', 'https://www.dropbox.com/s/9tqnqo930mpnpcn/256x256_diffusion_uncond.pt'] },\n    '512x512_diffusion_uncond_finetune_008100': { 'downloaded': False, 'sha': '9c111ab89e214862b76e1fa6a1b3f1d329b1a88281885943d2cdbe357ad57648', 'uri_list': ['https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt', 'https://huggingface.co/lowlevelware/512x512_diffusion_unconditional_ImageNet/resolve/main/512x512_diffusion_uncond_finetune_008100.pt'] },\n    'portrait_generator_v001': { 'downloaded': False, 'sha': 'b7e8c747af880d4480b6707006f1ace000b058dd0eac5bb13558ba3752d9b5b9', 'uri_list': ['https://huggingface.co/felipe3dartist/portrait_generator_v001/resolve/main/portrait_generator_v001_ema_0.9999_1MM.pt'] },\n    'portrait_generator_v1.5': { 'downloaded': False, 'sha': '3bc39e28fd9690dafbbee83cc08d089a3640eca8ed4d14280c4a9d342c56fd7f', 'uri_list': ['https://huggingface.co/felipe3dartist/portrait_generator_v1.5/resolve/main/ema_0.9999_165000.pt'] },\n    'pixelartdiffusion_expanded': { 'downloaded': False, 'sha': 'a73b40556634034bf43b5a716b531b46fb1ab890634d854f5bcbbef56838739a', 'uri_list': ['https://huggingface.co/KaliYuga/PADexpanded/resolve/main/PADexpanded.pt'] },\n    'pixel_art_diffusion_hard_256': { 'downloaded': False, 'sha': 'be4a9de943ec06eef32c65a1008c60ad017723a4d35dc13169c66bb322234161', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_hard_256/resolve/main/pixel_art_diffusion_hard_256.pt'] },\n    'pixel_art_diffusion_soft_256': { 'downloaded': False, 'sha': 'd321590e46b679bf6def1f1914b47c89e762c76f19ab3e3392c8ca07c791039c', 'uri_list': ['https://huggingface.co/KaliYuga/pixel_art_diffusion_soft_256/resolve/main/pixel_art_diffusion_soft_256.pt'] },\n    'pixelartdiffusion4k': { 'downloaded': False, 'sha': 'a1ba4f13f6dabb72b1064f15d8ae504d98d6192ad343572cc416deda7cccac30', 'uri_list': ['https://huggingface.co/KaliYuga/pixelartdiffusion4k/resolve/main/pixelartdiffusion4k.pt'] },\n    'watercolordiffusion_2': { 'downloaded': False, 'sha': '49c281b6092c61c49b0f1f8da93af9b94be7e0c20c71e662e2aa26fee0e4b1a9', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion_2/resolve/main/watercolordiffusion_2.pt'] },\n    'watercolordiffusion': { 'downloaded': False, 'sha': 'a3e6522f0c8f278f90788298d66383b11ac763dd5e0d62f8252c962c23950bd6', 'uri_list': ['https://huggingface.co/KaliYuga/watercolordiffusion/resolve/main/watercolordiffusion.pt'] },\n    'PulpSciFiDiffusion': { 'downloaded': False, 'sha': 'b79e62613b9f50b8a3173e5f61f0320c7dbb16efad42a92ec94d014f6e17337f', 'uri_list': ['https://huggingface.co/KaliYuga/PulpSciFiDiffusion/resolve/main/PulpSciFiDiffusion.pt'] },\n    'secondary': { 'downloaded': False, 'sha': '983e3de6f95c88c81b2ca7ebb2c217933be1973b1ff058776b970f901584613a', 'uri_list': ['https://the-eye.eu/public/AI/models/v-diffusion/secondary_model_imagenet_2.pth', 'https://ipfs.pollinations.ai/ipfs/bafybeibaawhhk7fhyhvmm7x24zwwkeuocuizbqbcg5nqx64jq42j75rdiy/secondary_model_imagenet_2.pth'] },\n    'liminal_diffusion_v1': { 'downloaded': False, 'sha': '87c36b544a367fceb0ca127d0028cd8a6f6b6e069e529b22999259d69c14f042', 'uri_list': ['https://huggingface.co/BrainArtLabs/liminal_diffusion/resolve/main/liminal_diffusion_v1.pt'] },\n    'liminal_diffusion_source': { 'downloaded': False, 'sha': 'ce0064b8cea56c8adb4e5aa0ee2d02f65cd8f1baa905cc6504f462f1aac6d6f4', 'uri_list': ['https://huggingface.co/BrainArtLabs/liminal_diffusion/resolve/main/liminal_diffusion_source.pt'] },\n    'medievaldiffusion': { 'downloaded': False, 'sha': '1b66a0c9749f88b2d9124af7a3ba5c12d2645ffcc33356326269ccda8643a01b', 'uri_list': ['https://huggingface.co/KaliYuga/medievaldiffusion/resolve/main/medievaldiffusion.pt'] },\n    'lithographydiffusion': { 'downloaded': False, 'sha': 'a3e6522f0c8f278f90788298d66383b11ac763dd5e0d62f8252c962c23950bd6', 'uri_list': ['https://huggingface.co/KaliYuga/lithographydiffusion/resolve/main/lithographydiffusion.pt'] },\n    'floral_diffusion': { 'downloaded': False, 'sha': '197e9068f1ca0248fd89f9d6ca1f6f851783e04fddd0cc5b0020bfa655ed3aeb', 'uri_list': ['https://huggingface.co/jags/floraldiffusion/resolve/main/floraldiffusion.pt'] },\n    'FeiArt_Handpainted_CG_Diffusion': { 'downloaded': False, 'sha': '85f95f0618f288476ffcec9f48160542ba626f655b3df963543388dcd059f86a', 'uri_list': ['https://huggingface.co/Feiart/FeiArt-Handpainted-CG-Diffusion/resolve/main/FeiArt-Handpainted-CG-Diffusion.pt'] },\n    'textilediffusion': { 'downloaded': False, 'sha': '82aa9ac10c67a806929b5399f04b933ffaa98f1a2bb0c18103d6ccd8f5bd2dd4', 'uri_list': ['https://huggingface.co/KaliYuga/textilediffusion/resolve/main/textilediffusion.pt'] },\n    'IsometricDiffusionRevrart512px': { 'downloaded': False, 'sha': '649bb7d10ea5170b71bc24adfb17f0305955fde38d2bb1bc5428c6d5baf9811c', 'uri_list': ['https://huggingface.co/Revrart/IsometricDiffusionRevrart512px/resolve/main/IsometricDiffusionRevrart512px.pt'] },\n}\n\nkaliyuga_pixel_art_model_names = ['pixelartdiffusion_expanded', 'pixel_art_diffusion_hard_256', 'pixel_art_diffusion_soft_256', 'pixelartdiffusion4k', 'PulpSciFiDiffusion']\nkaliyuga_wat_lit_tex_model_names = ['watercolordiffusion', 'watercolordiffusion_2', 'lithographydiffusion', 'textilediffusion'] #added, combined with lithography and textile models\nkaliyuga_pulpscifi_model_names = ['PulpSciFiDiffusion']\nkaliyuga_medieval_model_names = ['medievaldiffusion'] #added\nliminal_model_names = ['liminal_diffusion_v1', 'liminal_diffusion_source'] #added\njags_model_names = ['floral_diffusion'] #renamed\nfei_model_names = ['FeiArt_Handpainted_CG_Diffusion'] #added\ndiffusion_models_256x256_list = ['256x256_diffusion_uncond'] + kaliyuga_pixel_art_model_names + kaliyuga_wat_lit_tex_model_names + kaliyuga_pulpscifi_model_names + kaliyuga_medieval_model_names # + liminal_model_names\n\nfrom urllib.parse import urlparse\n\ndef get_model_filename(diffusion_model_name):\n    model_uri = diff_model_map[diffusion_model_name]['uri_list'][0]\n    model_filename = os.path.basename(urlparse(model_uri).path)\n    return model_filename\n\n# Download the diffusion model\ndef download_model(diffusion_model_name, uri_index=0):\n    if diffusion_model_name != 'custom':\n        model_filename = get_model_filename(diffusion_model_name)\n        model_local_path = os.path.join(model_path, model_filename)\n        if os.path.exists(model_local_path) and check_model_SHA:\n            print(f'Checking {diffusion_model_name} File')\n            with open(model_local_path, \"rb\") as f:\n                bytes = f.read() \n                hash = hashlib.sha256(bytes).hexdigest()\n            if hash == diff_model_map[diffusion_model_name]['sha']:\n                print(f'{diffusion_model_name} SHA matches')\n                diff_model_map[diffusion_model_name]['downloaded'] = True\n            else:\n                print(f\"{diffusion_model_name} SHA doesn't match. Will redownload it.\")\n        elif os.path.exists(model_local_path) and not check_model_SHA or diff_model_map[diffusion_model_name]['downloaded']:\n            print(f'{diffusion_model_name} already downloaded. If the file is corrupt, enable check_model_SHA.')\n            diff_model_map[diffusion_model_name]['downloaded'] = True\n\n        if not diff_model_map[diffusion_model_name]['downloaded']:\n            for model_uri in diff_model_map[diffusion_model_name]['uri_list']:\n                download_url(model_uri, model_path)\n                if os.path.exists(model_local_path):\n                    diff_model_map[diffusion_model_name]['downloaded'] = True\n                    return\n                else:\n                    print(f'{diffusion_model_name} model download from {model_uri} failed. Will try any fallback uri.')\n            print(f'{diffusion_model_name} download failed.')\n\n\n# Download the secondary diffusion model\ndownload_model(diffusion_model)\nif use_secondary_model:\n    download_model('secondary')\n\nmodel_config = model_and_diffusion_defaults()\nif diffusion_model == '512x512_diffusion_uncond_finetune_008100': #512x512 model\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n        'rescale_timesteps': True,\n        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n        'image_size': 512,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': not useCPU,\n        'use_scale_shift_norm': True,\n    })\nelif diffusion_model == '256x256_diffusion_uncond': #256x256 model\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': 1000, #No need to edit this, it is taken care of later.\n        'rescale_timesteps': True,\n        'timestep_respacing': 250, #No need to edit this, it is taken care of later.\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': not useCPU,\n        'use_scale_shift_norm': True,\n    })\nelif diffusion_model == 'portrait_generator_v001' or diffusion_model == 'portrait_generator_v1.5': #512x512 model\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'image_size': 512,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 4,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n    })\nelif diffusion_model == 'liminal_diffusion_v1' or diffusion_model == 'liminal_diffusion_source': #256x256 model\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n    })\nelif diffusion_model == 'lithographydiffusion': #256x256 model\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n    })\nelif diffusion_model == 'medievaldiffusion': #256x256 model\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n    })\nelif diffusion_model == 'floral_diffusion': #256x256 model\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n    })\nelif diffusion_model == 'FeiArt_Handpainted_CG_Diffusion': #512x512 model\n    model_config.update({\n        'attention_resolutions': '32,16,8',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 512,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n    })\nelif diffusion_model == 'textilediffusion': #256x256 model\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n    })\nelif diffusion_model == 'IsometricDiffusionRevrart512px': #512x512 model\n    model_config.update({\n        'attention_resolutions': '32, 16, 8',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 512,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 256,\n        'num_head_channels': 64,\n        'num_res_blocks': 2,\n        'resblock_updown': True,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': True,\n    })\nelse:  # E.g. A model finetuned by KaliYuga\n    model_config.update({\n        'attention_resolutions': '16',\n        'class_cond': False,\n        'diffusion_steps': 1000,\n        'rescale_timesteps': True,\n        'timestep_respacing': 'ddim100',\n        'image_size': 256,\n        'learn_sigma': True,\n        'noise_schedule': 'linear',\n        'num_channels': 128,\n        'num_heads': 1,\n        'num_res_blocks': 2,\n        'use_checkpoint': use_checkpoint,\n        'use_fp16': True,\n        'use_scale_shift_norm': False,\n      })\n\nmodel_default = model_config['image_size']\n\nif use_secondary_model:\n    secondary_model = SecondaryDiffusionImageNet2()\n    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n    secondary_model.eval().requires_grad_(False).to(device)\n\nclip_models = []\nif ViTB32: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device))\nif ViTB16: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device))\nif ViTL14: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device))\nif ViTL14_336px: clip_models.append(clip.load('ViT-L/14@336px', jit=False)[0].eval().requires_grad_(False).to(device))\nif RN50: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\nif RN50x4: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device))\nif RN50x16: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device))\nif RN50x64: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device))\nif RN101: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device))\nif ViTB32_laion2b_e16: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion2b_e16').eval().requires_grad_(False).to(device))\nif ViTB32_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\nif ViTB32_laion400m_32: clip_models.append(open_clip.create_model('ViT-B-32', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\nif ViTB32quickgelu_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\nif ViTB32quickgelu_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-32-quickgelu', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\nif ViTB16_laion400m_e31: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e31').eval().requires_grad_(False).to(device))\nif ViTB16_laion400m_e32: clip_models.append(open_clip.create_model('ViT-B-16', pretrained='laion400m_e32').eval().requires_grad_(False).to(device))\nif RN50_yffcc15m: clip_models.append(open_clip.create_model('RN50', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\nif RN50_cc12m: clip_models.append(open_clip.create_model('RN50', pretrained='cc12m').eval().requires_grad_(False).to(device))\nif RN50_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\nif RN50_quickgelu_cc12m: clip_models.append(open_clip.create_model('RN50-quickgelu', pretrained='cc12m').eval().requires_grad_(False).to(device))\nif RN101_yfcc15m: clip_models.append(open_clip.create_model('RN101', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\nif RN101_quickgelu_yfcc15m: clip_models.append(open_clip.create_model('RN101-quickgelu', pretrained='yfcc15m').eval().requires_grad_(False).to(device))\n\nnormalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\nlpips_model = lpips.LPIPS(net='vgg').to(device)","metadata":{"_uuid":"e2dd017f-7b74-48d0-b084-1418f525b424","_cell_guid":"69344948-25fa-41c0-9a72-f2a7d6982988","id":"ModelSettings","execution":{"iopub.status.busy":"2022-10-18T23:57:54.261798Z","iopub.execute_input":"2022-10-18T23:57:54.262154Z","iopub.status.idle":"2022-10-18T23:58:40.916322Z","shell.execute_reply.started":"2022-10-18T23:57:54.262119Z","shell.execute_reply":"2022-10-18T23:58:40.915185Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"512x512_diffusion_uncond_finetune_008100 already downloaded. If the file is corrupt, enable check_model_SHA.\nSetting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\nLoading model from: /opt/conda/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Custom model settings \nModify in accordance with your training settings and run the cell","metadata":{"_uuid":"8665aef2-702b-4352-80e7-9de67e3727ed","_cell_guid":"766b28aa-46df-46d9-b47a-ba2413073c97","id":"CustModelTop","trusted":true}},{"cell_type":"code","source":"#@markdown ####**Custom Model Settings:**\nif diffusion_model == 'custom':\n    model_config.update({\n          'attention_resolutions': '16',\n          'class_cond': False,\n          'diffusion_steps': 1000,\n          'rescale_timesteps': True,\n          'timestep_respacing': 'ddim100',\n          'image_size': 256,\n          'learn_sigma': True,\n          'noise_schedule': 'linear',\n          'num_channels': 128,\n          'num_heads': 1,\n          'num_res_blocks': 2,\n          'use_checkpoint': use_checkpoint,\n          'use_fp16': True,\n          'use_scale_shift_norm': False,\n    })","metadata":{"_uuid":"8682018d-a163-48f2-9405-2e0d9ef22b85","_cell_guid":"c962f252-3765-4a0e-bc51-20de8383b494","id":"CustModel","execution":{"iopub.status.busy":"2022-10-18T23:58:40.917896Z","iopub.execute_input":"2022-10-18T23:58:40.919931Z","iopub.status.idle":"2022-10-18T23:58:40.926489Z","shell.execute_reply.started":"2022-10-18T23:58:40.919890Z","shell.execute_reply":"2022-10-18T23:58:40.925367Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# 3. Settings","metadata":{"_uuid":"3c6c03f6-a31d-4b02-b2c2-23ad10599c38","_cell_guid":"3cca1b48-cadd-4c83-b986-6a7d27aa7e0a","id":"SettingsTop","trusted":true}},{"cell_type":"code","source":"#@markdown ####**Basic Settings:**\nbatch_name = 'FloralTest' #@param{type: 'string'}\nsteps = 250 #@param [25,50,100,150,240,250,260,500,1000]{type: 'raw', allow-input: true}\nclip_guidance_scale = 100000 #@param{type: 'number'}\ntv_scale = 25000 #@param{type: 'number'}\nrange_scale = 25000 #@param{type: 'number'}\nsat_scale = 25000 #@param{type: 'number'}\ncutn_batches = 6 #@param{type: 'number'}\nskip_augs = False #@param{type: 'boolean'}\n\n#@markdown ####**Only used for some KaliYuga_Ai models, consult custom model settings:**\nsat_scale_buffer = 0 #@param{type: 'number'}\nif sat_scale_buffer < 0.0 or sat_scale_buffer > 0.2:\n  sat_scale_buffer = 0\n  print('sat_scale_buffer out of range. Automatically reset to 0.0')\n\n#@markdown ####**Image dimensions for 512x512 models:**\n#@markdown **(512x512_uncond_finetuned, portrait_generator, FeiArt_Handpainted_CG, IsometricDiffusionRevrart512px)**\nwidth_height_for_512x512_models = [512, 704] #@param{type: 'raw'}\n\n#@markdown ####**Image dimensions for 256x256 models:**\n#@markdown **(256x256_uncond, pixart, pulpscifi, watercolor, lithography, medieval, textile, floral, liminal)**\nwidth_height_for_256x256_models = [512, 448] #@param{type: 'raw'}\n\n#@markdown ####**Video Init Basic Settings:**\nvideo_init_steps = 100 #@param [25,50,100,150,250,500,1000]{type: 'raw', allow-input: true}\nvideo_init_clip_guidance_scale = 1000 #@param{type: 'number'}\nvideo_init_tv_scale = 0.1#@param{type: 'number'}\nvideo_init_range_scale = 150#@param{type: 'number'}\nvideo_init_sat_scale = 300#@param{type: 'number'}\nvideo_init_cutn_batches = 4#@param{type: 'number'}\nvideo_init_skip_steps = 50 #@param{type: 'integer'}\n\n#@markdown ---\n\n#@markdown ####**Init Image Settings:**\ninit_image = 'None' #@param{type: 'string'}\ninit_scale = 1000 #@param{type: 'integer'}\nskip_steps = 10 #@param{type: 'integer'}\n#@markdown *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n\nwidth_height = width_height_for_256x256_models if diffusion_model in diffusion_models_256x256_list else width_height_for_512x512_models\n\n#Get corrected sizes in\nside_x = (width_height[0]//64)*64;\nside_y = (width_height[1]//64)*64;\nif side_x != width_height[0] or side_y != width_height[1]:\n    print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n\n#Make folder for batch\nbatchFolder = f'{outDirPath}/{batch_name}'\n#batchFolder = f'{outDirPath}/' # bad workaround for data_subdirs not refreshing in draft session, switched to FileLinks\ncreatePath(batchFolder) # path is /","metadata":{"_uuid":"c50f9d66-3dd8-465f-97f0-42dfc6588b8d","_cell_guid":"6e476b2c-7d54-48cc-87d9-68396696c7c9","id":"BasicSettings","execution":{"iopub.status.busy":"2022-10-18T23:58:40.928500Z","iopub.execute_input":"2022-10-18T23:58:40.928975Z","iopub.status.idle":"2022-10-18T23:58:40.942190Z","shell.execute_reply.started":"2022-10-18T23:58:40.928939Z","shell.execute_reply":"2022-10-18T23:58:40.941067Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Animation Settings","metadata":{"_uuid":"c3a7ea9e-4279-4c94-a149-e8599da506f0","_cell_guid":"4ce6dda8-a923-40db-a7e8-80fea4d39785","id":"AnimSetTop","trusted":true}},{"cell_type":"code","source":"#@markdown ####**Animation Mode:**\nanimation_mode = 'None' #@param ['None', '2D', '3D', 'Video Input'] {type:'string'}\n#@markdown *For animation, you probably want to turn `cutn_batches` to 1 to make it quicker.*\n\n\n#@markdown ---\n\n#@markdown ####**Video Input Settings:**\nvideo_init_path = \"init.mp4\" #@param {type: 'string'}\nextract_nth_frame = 2 #@param {type: 'number'}\npersistent_frame_output_in_batch_folder = True #@param {type: 'boolean'}\nvideo_init_seed_continuity = False #@param {type: 'boolean'}\n#@markdown #####**Video Optical Flow Settings:**\nvideo_init_flow_warp = True #@param {type: 'boolean'}\n# Call optical flow from video frames and warp prev frame with flow\nvideo_init_flow_blend =  0.999#@param {type: 'number'} #0 - take next frame, 1 - take prev warped frame\nvideo_init_check_consistency = False #Insert param here when ready\nvideo_init_blend_mode = \"optical flow\" #@param ['None', 'linear', 'optical flow']\n# Call optical flow from video frames and warp prev frame with flow\nif animation_mode == \"Video Input\":\n    if persistent_frame_output_in_batch_folder:\n        videoFramesFolder = f'{batchFolder}/videoFrames'\n    else:\n        videoFramesFolder = f'videoFrames'\n    createPath(videoFramesFolder)\n    print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n    try:\n        for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n            f.unlink()\n    except:\n        print('')\n    vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n    if os.path.exists(video_init_path):\n        subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel', 'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n    else: \n        print(f'\\nWARNING!\\n\\nVideo not found: {video_init_path}.\\nPlease check your video path.\\n')\n    #!ffmpeg -i {video_init_path} -vf {vf} -vsync vfr -q:v 2 -loglevel error -stats {videoFramesFolder}/%04d.jpg\n\n\n#@markdown ---\n\n#@markdown ####**2D Animation Settings:**\n#@markdown `zoom` is a multiplier of dimensions, 1 is no zoom.\n#@markdown All rotations are provided in degrees.\n\nkey_frames = True #@param {type:\"boolean\"}\nmax_frames = 10000#@param {type:\"number\"}\n\nif animation_mode == \"Video Input\":\n    max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n\ninterp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']{type:\"string\"}\nangle = \"0:(0)\"#@param {type:\"string\"}\nzoom = \"0: (1), 10: (1.05)\"#@param {type:\"string\"}\ntranslation_x = \"0: (0)\"#@param {type:\"string\"}\ntranslation_y = \"0: (0)\"#@param {type:\"string\"}\ntranslation_z = \"0: (10.0)\"#@param {type:\"string\"}\nrotation_3d_x = \"0: (0)\"#@param {type:\"string\"}\nrotation_3d_y = \"0: (0)\"#@param {type:\"string\"}\nrotation_3d_z = \"0: (0)\"#@param {type:\"string\"}\nmidas_depth_model = \"dpt_large\"#@param {type:\"string\"}\nmidas_weight = 0.3#@param {type:\"number\"}\nnear_plane = 200#@param {type:\"number\"}\nfar_plane = 10000#@param {type:\"number\"}\nfov = 40#@param {type:\"number\"}\npadding_mode = 'border'#@param {type:\"string\"}\nsampling_mode = 'bicubic'#@param {type:\"string\"}\n\n#======= TURBO MODE\n#@markdown ---\n#@markdown ####**Turbo Mode (3D anim only):**\n#@markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n#@markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames.\n#@markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n\nturbo_mode = False #@param {type:\"boolean\"}\nturbo_steps = \"2\" #@param [\"2\",\"3\",\"4\",\"5\",\"6\"] {type:\"string\"}\nturbo_preroll = 10 # frames\n\n#insist turbo be used only w 3d anim.\nif turbo_mode and animation_mode != '3D':\n    print('=====')\n    print('Turbo mode only available with 3D animations. Disabling Turbo.')\n    print('=====')\n    turbo_mode = False\n\n#@markdown ---\n\n#@markdown ####**Coherency Settings:**\n#@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\nframes_scale = 1500 #@param{type: 'integer'}\n#@markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\nframes_skip_steps = '60%' #@param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n\n#@markdown ####**Video Init Coherency Settings:**\n#@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\nvideo_init_frames_scale = 15000 #@param{type: 'integer'}\n#@markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\nvideo_init_frames_skip_steps = '70%' #@param ['40%', '50%', '60%', '70%', '80%'] {type: 'string'}\n\n#======= VR MODE\n#@markdown ---\n#@markdown ####**VR Mode (3D anim only):**\n#@markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.   \n#@markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n#@markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n#@markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n#@markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n#@markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n#@markdown \n#@markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n\nvr_mode = False #@param {type:\"boolean\"}\n#@markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\nvr_eye_angle = 0.5 #@param{type:\"number\"}\n#@markdown interpupillary distance (between the eyes)\nvr_ipd = 5.0 #@param{type:\"number\"}\n\n#insist VR be used only w 3d anim.\nif vr_mode and animation_mode != '3D':\n    print('=====')\n    print('VR mode only available with 3D animations. Disabling VR.')\n    print('=====')\n    vr_mode = False\n\n\ndef parse_key_frames(string, prompt_parser=None):\n    \"\"\"Given a string representing frame numbers paired with parameter values at that frame,\n    return a dictionary with the frame numbers as keys and the parameter values as the values.\n\n    Parameters\n    ----------\n    string: string\n        Frame numbers paired with parameter values at that frame number, in the format\n        'framenumber1: (parametervalues1), framenumber2: (parametervalues2), ...'\n    prompt_parser: function or None, optional\n        If provided, prompt_parser will be applied to each string of parameter values.\n    \n    Returns\n    -------\n    dict\n        Frame numbers as keys, parameter values at that frame number as values\n\n    Raises\n    ------\n    RuntimeError\n        If the input string does not match the expected format.\n    \n    Examples\n    --------\n    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\")\n    {10: 'Apple: 1| Orange: 0', 20: 'Apple: 0| Orange: 1| Peach: 1'}\n\n    >>> parse_key_frames(\"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\", prompt_parser=lambda x: x.lower()))\n    {10: 'apple: 1| orange: 0', 20: 'apple: 0| orange: 1| peach: 1'}\n    \"\"\"\n    import re\n    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n    frames = dict()\n    for match_object in re.finditer(pattern, string):\n        frame = int(match_object.groupdict()['frame'])\n        param = match_object.groupdict()['param']\n        if prompt_parser:\n            frames[frame] = prompt_parser(param)\n        else:\n            frames[frame] = param\n\n    if frames == {} and len(string) != 0:\n        raise RuntimeError('Key Frame string not correctly formatted')\n    return frames\n\ndef get_inbetweens(key_frames, integer=False):\n    \"\"\"Given a dict with frame numbers as keys and a parameter value as values,\n    return a pandas Series containing the value of the parameter at every frame from 0 to max_frames.\n    Any values not provided in the input dict are calculated by linear interpolation between\n    the values of the previous and next provided frames. If there is no previous provided frame, then\n    the value is equal to the value of the next provided frame, or if there is no next provided frame,\n    then the value is equal to the value of the previous provided frame. If no frames are provided,\n    all frame values are NaN.\n\n    Parameters\n    ----------\n    key_frames: dict\n        A dict with integer frame numbers as keys and numerical values of a particular parameter as values.\n    integer: Bool, optional\n        If True, the values of the output series are converted to integers.\n        Otherwise, the values are floats.\n    \n    Returns\n    -------\n    pd.Series\n        A Series with length max_frames representing the parameter values for each frame.\n    \n    Examples\n    --------\n    >>> max_frames = 5\n    >>> get_inbetweens({1: 5, 3: 6})\n    0    5.0\n    1    5.0\n    2    5.5\n    3    6.0\n    4    6.0\n    dtype: float64\n\n    >>> get_inbetweens({1: 5, 3: 6}, integer=True)\n    0    5\n    1    5\n    2    5\n    3    6\n    4    6\n    dtype: int64\n    \"\"\"\n    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n\n    for i, value in key_frames.items():\n        key_frame_series[i] = value\n    key_frame_series = key_frame_series.astype(float)\n    \n    interp_method = interp_spline\n\n    if interp_method == 'Cubic' and len(key_frames.items()) <=3:\n        interp_method = 'Quadratic'\n    \n    if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n        interp_method = 'Linear'\n    \n    key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n    key_frame_series[max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n    # key_frame_series = key_frame_series.interpolate(method=intrp_method,order=1, limit_direction='both')\n    key_frame_series = key_frame_series.interpolate(method=interp_method.lower(),limit_direction='both')\n    if integer:\n        return key_frame_series.astype(int)\n    return key_frame_series\n\ndef split_prompts(prompts):\n    prompt_series = pd.Series([np.nan for a in range(max_frames)])\n    for i, prompt in prompts.items():\n        prompt_series[i] = prompt\n    # prompt_series = prompt_series.astype(str)\n    prompt_series = prompt_series.ffill().bfill()\n    return prompt_series\n\nif key_frames:\n    try:\n        angle_series = get_inbetweens(parse_key_frames(angle))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `angle` correctly for key frames.\\n\"\n            \"Attempting to interpret `angle` as \"\n            f'\"0: ({angle})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        angle = f\"0: ({angle})\"\n        angle_series = get_inbetweens(parse_key_frames(angle))\n\n    try:\n        zoom_series = get_inbetweens(parse_key_frames(zoom))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `zoom` correctly for key frames.\\n\"\n            \"Attempting to interpret `zoom` as \"\n            f'\"0: ({zoom})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        zoom = f\"0: ({zoom})\"\n        zoom_series = get_inbetweens(parse_key_frames(zoom))\n\n    try:\n        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `translation_x` correctly for key frames.\\n\"\n            \"Attempting to interpret `translation_x` as \"\n            f'\"0: ({translation_x})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        translation_x = f\"0: ({translation_x})\"\n        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n\n    try:\n        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `translation_y` correctly for key frames.\\n\"\n            \"Attempting to interpret `translation_y` as \"\n            f'\"0: ({translation_y})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        translation_y = f\"0: ({translation_y})\"\n        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n\n    try:\n        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `translation_z` correctly for key frames.\\n\"\n            \"Attempting to interpret `translation_z` as \"\n            f'\"0: ({translation_z})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        translation_z = f\"0: ({translation_z})\"\n        translation_z_series = get_inbetweens(parse_key_frames(translation_z))\n\n    try:\n        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `rotation_3d_x` correctly for key frames.\\n\"\n            \"Attempting to interpret `rotation_3d_x` as \"\n            f'\"0: ({rotation_3d_x})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        rotation_3d_x = f\"0: ({rotation_3d_x})\"\n        rotation_3d_x_series = get_inbetweens(parse_key_frames(rotation_3d_x))\n\n    try:\n        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `rotation_3d_y` correctly for key frames.\\n\"\n            \"Attempting to interpret `rotation_3d_y` as \"\n            f'\"0: ({rotation_3d_y})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        rotation_3d_y = f\"0: ({rotation_3d_y})\"\n        rotation_3d_y_series = get_inbetweens(parse_key_frames(rotation_3d_y))\n\n    try:\n        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n    except RuntimeError as e:\n        print(\n            \"WARNING: You have selected to use key frames, but you have not \"\n            \"formatted `rotation_3d_z` correctly for key frames.\\n\"\n            \"Attempting to interpret `rotation_3d_z` as \"\n            f'\"0: ({rotation_3d_z})\"\\n'\n            \"Please read the instructions to find out how to use key frames \"\n            \"correctly.\\n\"\n        )\n        rotation_3d_z = f\"0: ({rotation_3d_z})\"\n        rotation_3d_z_series = get_inbetweens(parse_key_frames(rotation_3d_z))\n\nelse:\n    angle = float(angle)\n    zoom = float(zoom)\n    translation_x = float(translation_x)\n    translation_y = float(translation_y)\n    translation_z = float(translation_z)\n    rotation_3d_x = float(rotation_3d_x)\n    rotation_3d_y = float(rotation_3d_y)\n    rotation_3d_z = float(rotation_3d_z)","metadata":{"_uuid":"aed88952-ac56-465e-bc7b-97dfc7034b1f","_cell_guid":"eb5ea1dc-4854-4be6-b240-e359c6b86ec2","id":"AnimSettings","execution":{"iopub.status.busy":"2022-10-18T23:58:40.947111Z","iopub.execute_input":"2022-10-18T23:58:40.947457Z","iopub.status.idle":"2022-10-18T23:58:41.021147Z","shell.execute_reply.started":"2022-10-18T23:58:40.947430Z","shell.execute_reply":"2022-10-18T23:58:41.020271Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#@title Install RAFT for Video input animation mode only\n#@markdown Run once per session. Doesn't download again if model path exists.\n#@markdown Use force download to reload raft models if needed\nforce_download = False #@param {type:'boolean'}\nif animation_mode == 'Video Input':\n    try:\n        from raft import RAFT\n    except:\n        if not os.path.exists(os.path.join(PROJECT_DIR, 'RAFT')):\n            gitclone('https://github.com/princeton-vl/RAFT', os.path.join(PROJECT_DIR, 'RAFT'))\n        sys.path.append(f'{PROJECT_DIR}/RAFT')\n\n    if (not (os.path.exists(f'{root_path}/RAFT/models'))) or force_download:\n        createPath(f'{root_path}/RAFT')\n        os.chdir(f'{root_path}/RAFT')\n        sub_p_res = subprocess.run(['bash', f'{PROJECT_DIR}/RAFT/download_models.sh'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n        print(sub_p_res)\n        os.chdir(PROJECT_DIR)","metadata":{"_uuid":"c35fac2f-f4e5-4d57-800d-c470d198c0bf","_cell_guid":"0597dac2-d481-4e7d-bd68-437885ce0208","id":"InstallRAFT","execution":{"iopub.status.busy":"2022-10-18T23:58:41.022637Z","iopub.execute_input":"2022-10-18T23:58:41.023017Z","iopub.status.idle":"2022-10-18T23:58:41.032493Z","shell.execute_reply.started":"2022-10-18T23:58:41.022984Z","shell.execute_reply":"2022-10-18T23:58:41.031489Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#@title Define optical flow functions for Video input animation mode only\nif animation_mode == 'Video Input':\n    in_path = videoFramesFolder\n    flo_folder = f'{in_path}/out_flo_fwd'\n    path = f'{PROJECT_DIR}/RAFT/core'\n    import sys\n    sys.path.append(f'{PROJECT_DIR}/RAFT/core')\n    os.chdir(f'{PROJECT_DIR}/RAFT/core')\n    print(os.getcwd())\n  \n    print(\"Renaming RAFT core's utils.utils to raftutils.utils (to avoid a naming conflict with AdaBins)\")\n    if not os.path.exists(f'{PROJECT_DIR}/RAFT/core/raftutils'):\n        os.rename(f'{PROJECT_DIR}/RAFT/core/utils', f'{PROJECT_DIR}/RAFT/core/raftutils')\n        sub_p_res = subprocess.run(['sed', '-i', 's/from utils.utils/from raftutils.utils/g', f'{PROJECT_DIR}/RAFT/core/corr.py'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n        sub_p_res = subprocess.run(['sed', '-i', 's/from utils.utils/from raftutils.utils/g', f'{PROJECT_DIR}/RAFT/core/raft.py'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n\n    from raftutils.utils import InputPadder\n    from raft import RAFT\n    from raftutils import flow_viz\n    import numpy as np\n    import argparse, PIL, cv2\n    from PIL import Image\n    from tqdm.notebook import tqdm\n    from glob import glob\n    import torch\n  \n    args2 = argparse.Namespace()\n    args2.small = False\n    args2.mixed_precision = True\n  \n  \n    TAG_CHAR = np.array([202021.25], np.float32)\n  \n    def writeFlow(filename,uv,v=None):\n        \"\"\" \n        https://github.com/NVIDIA/flownet2-pytorch/blob/master/utils/flow_utils.py\n        Copyright 2017 NVIDIA CORPORATION\n  \n        Licensed under the Apache License, Version 2.0 (the \"License\");\n        you may not use this file except in compliance with the License.\n        You may obtain a copy of the License at\n  \n            http://www.apache.org/licenses/LICENSE-2.0\n  \n        Unless required by applicable law or agreed to in writing, software\n        distributed under the License is distributed on an \"AS IS\" BASIS,\n        WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n        See the License for the specific language governing permissions and\n        limitations under the License.\n        \n        Write optical flow to file.\n        \n        If v is None, uv is assumed to contain both u and v channels,\n        stacked in depth.\n        Original code by Deqing Sun, adapted from Daniel Scharstein.\n        \"\"\"\n        nBands = 2\n  \n        if v is None:\n            assert(uv.ndim == 3)\n            assert(uv.shape[2] == 2)\n            u = uv[:,:,0]\n            v = uv[:,:,1]\n        else:\n            u = uv\n  \n        assert(u.shape == v.shape)\n        height,width = u.shape\n        f = open(filename,'wb')\n        # write the header\n        f.write(TAG_CHAR)\n        np.array(width).astype(np.int32).tofile(f)\n        np.array(height).astype(np.int32).tofile(f)\n        # arrange into matrix form\n        tmp = np.zeros((height, width*nBands))\n        tmp[:,np.arange(width)*2] = u\n        tmp[:,np.arange(width)*2 + 1] = v\n        tmp.astype(np.float32).tofile(f)\n        f.close()\n  \n    def load_img(img, size):\n        img = Image.open(img).convert('RGB').resize(size)\n        return torch.from_numpy(np.array(img)).permute(2,0,1).float()[None,...].cuda()\n  \n    def get_flow(frame1, frame2, model, iters=20):\n        padder = InputPadder(frame1.shape)\n        frame1, frame2 = padder.pad(frame1, frame2)\n        _, flow12 = model(frame1, frame2, iters=iters, test_mode=True)\n        flow12 = flow12[0].permute(1, 2, 0).detach().cpu().numpy()\n  \n        return flow12\n  \n    def warp_flow(img, flow):\n        h, w = flow.shape[:2]\n        flow = flow.copy()\n        flow[:, :, 0] += np.arange(w)\n        flow[:, :, 1] += np.arange(h)[:, np.newaxis]\n        res = cv2.remap(img, flow, None, cv2.INTER_LINEAR)\n        return res\n  \n    def makeEven(_x):\n        return _x if (_x % 2 == 0) else _x+1\n  \n    def fit(img,maxsize=512):\n        maxdim = max(*img.size)\n        if maxdim>maxsize:\n            # if True:\n            ratio = maxsize/maxdim\n            x,y = img.size\n            size = (makeEven(int(x*ratio)),makeEven(int(y*ratio))) \n            img = img.resize(size)\n        return img\n  \n    def warp(frame1, frame2, flo_path, blend=0.5, weights_path=None):\n        flow21 = np.load(flo_path)\n        frame1pil = np.array(frame1.convert('RGB').resize((flow21.shape[1],flow21.shape[0])))\n        frame1_warped21 = warp_flow(frame1pil, flow21)\n        # frame2pil = frame1pil\n        frame2pil = np.array(frame2.convert('RGB').resize((flow21.shape[1],flow21.shape[0])))\n    \n        if weights_path:\n            # TBD\n            pass\n        else:\n            blended_w = frame2pil*(1-blend) + frame1_warped21*(blend)\n  \n        return  PIL.Image.fromarray(blended_w.astype('uint8'))\n  \n    in_path = videoFramesFolder\n    flo_folder = f'{in_path}/out_flo_fwd'\n  \n    temp_flo = in_path+'/temp_flo'\n    flo_fwd_folder = in_path+'/out_flo_fwd'\n    # TBD flow backwards!\n  \n    os.chdir(PROJECT_DIR)","metadata":{"_uuid":"46b17d39-c2e1-4df1-89b7-175f00a559ca","_cell_guid":"9dee1297-bf62-4b94-90b5-47c2ca32b009","id":"FlowFns1","execution":{"iopub.status.busy":"2022-10-18T23:58:41.034267Z","iopub.execute_input":"2022-10-18T23:58:41.035772Z","iopub.status.idle":"2022-10-18T23:58:41.059995Z","shell.execute_reply.started":"2022-10-18T23:58:41.035733Z","shell.execute_reply":"2022-10-18T23:58:41.058959Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#@title Generate optical flow and consistency maps\n#@markdown Run once per init video\n\nif animation_mode == \"Video Input\":\n    import gc\n\n    force_flow_generation = False #@param {type:'boolean'}\n    in_path = videoFramesFolder\n    flo_folder = f'{in_path}/out_flo_fwd'\n\n    if not video_init_flow_warp:\n        print('video_init_flow_warp not set, skipping')\n\n    if (animation_mode == 'Video Input') and (video_init_flow_warp):\n        flows = glob(flo_folder+'/*.*')\n        if (len(flows)>0) and not force_flow_generation:\n            print(f'Skipping flow generation:\\nFound {len(flows)} existing flow files in current working folder: {flo_folder}.\\nIf you wish to generate new flow files, check force_flow_generation and run this cell again.')\n    \n        if (len(flows)==0) or force_flow_generation:\n            frames = sorted(glob(in_path+'/*.*'));\n            if len(frames)<2: \n                print(f'WARNING!\\nCannot create flow maps: Found {len(frames)} frames extracted from your video input.\\nPlease check your video path.')\n            if len(frames)>=2:\n        \n                raft_model = torch.nn.DataParallel(RAFT(args2))\n                raft_model.load_state_dict(torch.load(f'{root_path}/RAFT/models/raft-things.pth'))\n                raft_model = raft_model.module.cuda().eval()\n        \n                for f in pathlib.Path(f'{flo_fwd_folder}').glob('*.*'):\n                    f.unlink()\n        \n                temp_flo = in_path+'/temp_flo'\n                flo_fwd_folder = in_path+'/out_flo_fwd'\n        \n                createPath(flo_fwd_folder)\n                createPath(temp_flo)\n        \n                # TBD Call out to a consistency checker?\n        \n                framecount = 0\n                for frame1, frame2 in tqdm(zip(frames[:-1], frames[1:]), total=len(frames)-1):\n        \n                    out_flow21_fn = f\"{flo_fwd_folder}/{frame1.split('/')[-1]}\"\n            \n                    frame1 = load_img(frame1, width_height)\n                    frame2 = load_img(frame2, width_height)\n            \n                    flow21 = get_flow(frame2, frame1, raft_model)\n                    np.save(out_flow21_fn, flow21)\n            \n                    if video_init_check_consistency:\n                        # TBD\n                        pass\n\n                del raft_model \n                gc.collect()","metadata":{"_uuid":"c48b02ec-c7de-41c5-a1b4-5ad08bcd415b","_cell_guid":"4efc8de0-92d5-45f8-928f-9f5cc6583ab4","id":"FlowFns2","execution":{"iopub.status.busy":"2022-10-18T23:58:41.061782Z","iopub.execute_input":"2022-10-18T23:58:41.062126Z","iopub.status.idle":"2022-10-18T23:58:41.075614Z","shell.execute_reply.started":"2022-10-18T23:58:41.062092Z","shell.execute_reply":"2022-10-18T23:58:41.074268Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Extra Settings\n Partial Saves, Advanced Settings, Cutn Scheduling","metadata":{"_uuid":"97942449-8b04-4fa2-82fb-1b1967304036","_cell_guid":"99097982-2bbf-4805-afc6-74c1450850ce","id":"ExtraSetTop","trusted":true}},{"cell_type":"code","source":"#@markdown ####**Saving:**\n\nintermediate_saves = 0#@param{type: 'raw'}\nintermediates_in_subfolder = True #@param{type: 'boolean'}\n#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n\n#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n\n#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n\n\nif type(intermediate_saves) is not list:\n    if intermediate_saves:\n        steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n        steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n        print(f'Will save every {steps_per_checkpoint} steps')\n    else:\n        steps_per_checkpoint = steps+10\nelse:\n    steps_per_checkpoint = None\n\nif intermediate_saves and intermediates_in_subfolder is True:\n    partialFolder = f'{batchFolder}/partials'\n    createPath(partialFolder)\n\n#@markdown ---\n\n#@markdown ####**Advanced Settings:**\n#@markdown *There are a few extra advanced settings available if you double click this cell.*\n\n#@markdown *Perlin init will replace your init, so uncheck if using one.*\n\nperlin_init = False  #@param{type: 'boolean'}\nperlin_mode = 'mixed' #@param ['mixed', 'color', 'gray']\nset_seed = '1557011537' #@param{type: 'string'}\neta = 0.9 #@param{type: 'number'}\nclamp_grad = True #@param{type: 'boolean'}\nclamp_max = 0.25 #@param{type: 'number'}\n\n#@markdown ### Extra advanced settings:\nrandomize_class = True #@param{type: 'boolean'}\nclip_denoised = False #@param{type: 'boolean'}\nfuzzy_prompt = False #@param{type: 'boolean'}\nrand_mag = 0.25 #@param{type: 'number'}\n\n\n#@markdown ---\n\n#@markdown ####**Cutn Scheduling:**\n#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n\n#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n\n#@markdown **For correct settings consult default or custom model settings, depends on selected model**\ncut_overview = \"[11]*200+[9]*200+[4]*200+[2]*200+[0]*200\" #@param {type: 'string'}\ncut_innercut = \"[0]*200+[2]*200+[7]*200+[9]*200+[10]*200\" #@param {type: 'string'}\ncut_ic_pow = \"[0]*200+[10]*200+[20]*200+[10]*200+[10]*200\" #@param {type: 'string'}\ncut_icgray_p = \"[0.2]*400+[0]*600\" #@param {type: 'string'}\n\n#@markdown **KaliYuga model settings. Refer to [cut_ic_pow](https://ezcharts.miraheze.org/wiki/Category:Cut_ic_pow) as a guide. Values between 1 and 100 all work.**\npad_or_pulp_cut_overview = \"[15]*100+[15]*100+[12]*100+[12]*100+[6]*100+[4]*100+[2]*200+[0]*200\" #@param {type: 'string'}\npad_or_pulp_cut_innercut = \"[1]*100+[1]*100+[4]*100+[4]*100+[8]*100+[8]*100+[10]*200+[10]*200\" #@param {type: 'string'}\npad_or_pulp_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'}\npad_or_pulp_cut_icgray_p = \"[0.87]*100+[0.78]*50+[0.73]*50+[0.64]*60+[0.56]*40+[0.50]*50+[0.33]*100+[0.19]*150+[0]*400\" #@param {type: 'string'}\n\n#@markdown **Used for Kaliyuga watercolor, lithography and textile diffusion models**\nwat_lit_tex_cut_overview = \"[14]*200+[12]*200+[4]*400+[0]*200\" #@param {type: 'string'} #renamed\nwat_lit_tex_cut_innercut = \"[2]*200+[4]*200+[12]*400+[12]*200\" #@param {type: 'string'} #renamed\nwat_lit_tex_cut_ic_pow = \"[12]*300+[12]*100+[12]*50+[12]*50+[10]*100+[10]*100+[10]*300\" #@param {type: 'string'} #renamed\nwat_lit_tex_cut_icgray_p = \"[0.7]*100+[0.6]*100+[0.45]*100+[0.3]*100+[0]*600\" #@param {type: 'string'} #renamed\n\n#@markdown **Used for Kaliyuga medieval model**\nmed_cut_overview = \"[14]*200+[12]*200+[4]*400+[0]*200\" #@param {type: 'string'} #added\nmed_cut_innercut =\"[2]*200+[4]*200+[12]*400+[12]*200\" #@param {type: 'string'} #added\nmed_cut_ic_pow = \"[12]*1000\" #@param {type: 'string'} #added\nmed_cut_icgray_p = \"[0.7]*100+[0.6]*100+[0.45]*100+[0.3]*100+[0]*600\" #@param {type: 'string'} #added\n\n#@markdown **Used for liminal diffusion models**\nlim_cut_overview = \"[16]*400+[4]*600\" #@param {type: 'string'} #added\nlim_cut_innercut =\"[4]*400+[16]*600\" #@param {type: 'string'} #added\nlim_cut_ic_pow = \"2\" #@param {type: 'string'} #added\nlim_cut_icgray_p = \"[0.2]*400+[0.1]*600\" #@param {type: 'string'} #added\n\n#@markdown **Used for Jags111 floral diffusion model**\njag_cut_overview = \"[12]*400+[4]*600\" #@param {type: 'string'} #added\njag_cut_innercut = \"[4]*400+[12]*600\" #@param {type: 'string'} #added\njag_cut_ic_pow = \"[1]*1000\" #@param {type: 'string'} #added\njag_cut_icgray_p = \"[0.2]*400+[0]*600\" #@param {type: 'string'} #added\n\n#@markdown **Used for Jags111 floral diffusion model**\nfei_cut_overview = \"[12]*400+[4]*600\" #@param {type: 'string'} #added\nfei_cut_innercut = \"[4]*400+[12]*600\" #@param {type: 'string'} #added\nfei_cut_ic_pow = \"[1]*1000\" #@param {type: 'string'} #added\nfei_cut_icgray_p = \"[0.6]*200+[0.4]*200+[0.2]*200+[0.1]*200+[0]*200\" #@param {type: 'string'} #added\n\nif (diffusion_model in kaliyuga_pixel_art_model_names) or (diffusion_model in kaliyuga_pulpscifi_model_names):\n    cut_overview = pad_or_pulp_cut_overview\n    cut_innercut = pad_or_pulp_cut_innercut\n    cut_ic_pow = pad_or_pulp_cut_ic_pow\n    cut_icgray_p = pad_or_pulp_cut_icgray_p\nelif diffusion_model in kaliyuga_wat_lit_tex_model_names: #renamed\n    cut_overview = wat_lit_tex_cut_overview\n    cut_innercut = wat_lit_tex_cut_innercut\n    cut_ic_pow = wat_lit_tex_cut_ic_pow\n    cut_icgray_p = wat_lit_tex_cut_icgray_p\nelif diffusion_model in kaliyuga_medieval_model_names: #added\n    cut_overview = med_cut_overview\n    cut_innercut = med_cut_innercut\n    cut_ic_pow = med_cut_ic_pow\n    cut_icgray_p = med_cut_icgray_p\nelif diffusion_model in jags_model_names: #added\n    cut_overview = jag_cut_overview\n    cut_innercut = jag_cut_innercut\n    cut_ic_pow = jag_cut_ic_pow\n    cut_icgray_p = jag_cut_icgray_p\nelif diffusion_model in fei_model_names: #added\n    cut_overview = fei_cut_overview\n    cut_innercut = fei_cut_innercut\n    cut_ic_pow = fei_cut_ic_pow\n    cut_icgray_p = fei_cut_icgray_p\nelif diffusion_model in liminal_model_names: #added\n    cut_overview = lim_cut_overview\n    cut_innercut = lim_cut_innercut\n    cut_ic_pow = lim_cut_ic_pow\n    cut_icgray_p = lim_cut_icgray_p\n\n#@markdown ---\n\n#@markdown ####**Transformation Settings:**\nuse_vertical_symmetry = False #@param {type:\"boolean\"}\nuse_horizontal_symmetry = False #@param {type:\"boolean\"}\ntransformation_percent = [0.09] #@param","metadata":{"_uuid":"5235d9bd-01ae-4c4f-969e-b01a6d296841","_cell_guid":"3ac082e1-0c61-489b-89be-3264909399e8","id":"ExtraSettings","execution":{"iopub.status.busy":"2022-10-18T23:58:41.079115Z","iopub.execute_input":"2022-10-18T23:58:41.079546Z","iopub.status.idle":"2022-10-18T23:58:41.097581Z","shell.execute_reply.started":"2022-10-18T23:58:41.079493Z","shell.execute_reply":"2022-10-18T23:58:41.096701Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Prompts\n`animation_mode: None` will only use the first set. `animation_mode: 2D / Video` will run through them per the set frames and hold on the last one.","metadata":{"_uuid":"340129f3-ddb7-445f-9475-e13f603c1e65","_cell_guid":"6eb4e1f6-1c4c-4970-bace-b141bde00594","id":"PromptsTop","trusted":true}},{"cell_type":"code","source":"# Note: If using a pixelart diffusion model, try adding \"#pixelart\" to the end of the prompt for a stronger effect. It'll tend to work a lot better!\n\n# Prompt for default 5.6\ntext_prompts = {\n#0: [\"A photo-real delicate sculpture of an ornate detailed Phoenix in front of a intricate background by Courtney Karp, micro detail, backlit lighting, octane renderer, colorful, physically based rendering, tribal art, trending on cgsociety:2\",\n#            \"text, words, symmetrical features, centered composition, signatures:-1\",\n#            \"gold and red lines.\",\n#            \"delicate porcelain, cgsociety.\"],\n0: [\"beautiful, extremely detailed photorealistic glossy porcelain cat, symmetric, fractal art, fine, subsurface scattering, beeple, colourful, red, gold, trending on artstation, ornaments, swirls, ornate, unreal enginebeautiful extremly detailed glossy intricate porcelain dragon made with fractals by peter gric and ellen jewett:3\",\n            \"fractalism:2\",\n            \"fractal design:2\",\n            \"mandelbrot\",\n            \"ornaments\",\n            \"beeple\",\n            \"cgsocietyintricate\",\n            \"3d render\",\n            \"trending on artstation\"],\n}            \n\nimage_prompts = {\n    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n}","metadata":{"_uuid":"c6d11d9f-0e70-49d7-b3fa-48abe09fb9ec","_cell_guid":"297732cb-42c4-49f6-bff7-9180f7bf5b7d","id":"Prompts","execution":{"iopub.status.busy":"2022-10-18T23:58:41.099303Z","iopub.execute_input":"2022-10-18T23:58:41.100030Z","iopub.status.idle":"2022-10-18T23:58:41.112212Z","shell.execute_reply.started":"2022-10-18T23:58:41.099995Z","shell.execute_reply":"2022-10-18T23:58:41.111350Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# 4. Diffuse!","metadata":{"_uuid":"6137e0cf-8e0c-41d2-9a0b-4f2145e8ac8e","_cell_guid":"bfccb6be-d346-444a-836e-465b805f05c5","id":"DiffuseTop","trusted":true}},{"cell_type":"code","source":"#@title Do the Run!\n#@markdown `n_batches` ignored with animation modes.\ndisplay_rate = 15 #@param{type: 'number'}\nn_batches = 1 #@param{type: 'number'}\n\nif animation_mode == 'Video Input':\n    steps = video_init_steps\n\n#Update Model Settings\ntimestep_respacing = f'ddim{steps}'\ndiffusion_steps = (1000//steps)*steps if steps < 1000 else steps\nmodel_config.update({\n    'timestep_respacing': timestep_respacing,\n    'diffusion_steps': diffusion_steps,\n})\n\nbatch_size = 1 \n\ndef move_files(start_num, end_num, old_folder, new_folder):\n    for i in range(start_num, end_num):\n        old_file = old_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n        new_file = new_folder + f'/{batch_name}({batchNum})_{i:04}.png'\n        os.rename(old_file, new_file)\n\n#@markdown ---\n\n\nresume_run = False #@param{type: 'boolean'}\nrun_to_resume = 'latest' #@param{type: 'string'}\nresume_from_frame = 'latest' #@param{type: 'string'}\nretain_overwritten_frames = False #@param{type: 'boolean'}\nif retain_overwritten_frames:\n    retainFolder = f'{batchFolder}/retained'\n    createPath(retainFolder)\n\n\nskip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\ncalc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n\nif animation_mode == 'Video Input':\n    frames = sorted(glob(in_path+'/*.*'));\n    if len(frames)==0: \n        sys.exit(\"ERROR: 0 frames found.\\nPlease check your video input path and rerun the video settings cell.\")\n    flows = glob(flo_folder+'/*.*')\n    if (len(flows)==0) and video_init_flow_warp:\n        sys.exit(\"ERROR: 0 flow files found.\\nPlease rerun the flow generation cell.\")\n\nif steps <= calc_frames_skip_steps:\n    sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n\nif resume_run:\n    if run_to_resume == 'latest':\n        try:\n            batchNum\n        except:\n            batchNum = len(glob(f\"{batchFolder}/{batch_name}(*)_settings.txt\"))-1\n    else:\n        batchNum = int(run_to_resume)\n    if resume_from_frame == 'latest':\n        start_frame = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n        if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n            start_frame = start_frame - (start_frame % int(turbo_steps))\n    else:\n        start_frame = int(resume_from_frame)+1\n        if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n            start_frame = start_frame - (start_frame % int(turbo_steps))\n        if retain_overwritten_frames is True:\n            existing_frames = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n            frames_to_save = existing_frames - start_frame\n            print(f'Moving {frames_to_save} frames to the Retained folder')\n            move_files(start_frame, existing_frames, batchFolder, retainFolder)\nelse:\n    start_frame = 0\n    batchNum = len(glob(batchFolder+\"/*.txt\"))\n    while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\"):\n        batchNum += 1\n\nprint(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n\nif set_seed == 'random_seed':\n    random.seed()\n    seed = random.randint(0, 2**32)\n    # print(f'Using seed: {seed}')\nelse:\n    seed = int(set_seed)\n\nargs = {\n    'batchNum': batchNum,\n    'prompts_series':split_prompts(text_prompts) if text_prompts else None,\n    'image_prompts_series':split_prompts(image_prompts) if image_prompts else None,\n    'seed': seed,\n    'display_rate':display_rate,\n    'n_batches':n_batches if animation_mode == 'None' else 1,\n    'batch_size':batch_size,\n    'batch_name': batch_name,\n    'steps': steps,\n    'diffusion_sampling_mode': diffusion_sampling_mode,\n    'width_height': width_height,\n    'clip_guidance_scale': clip_guidance_scale,\n    'tv_scale': tv_scale,\n    'range_scale': range_scale,\n    'sat_scale': sat_scale,\n    'cutn_batches': cutn_batches,\n    'init_image': init_image,\n    'init_scale': init_scale,\n    'skip_steps': skip_steps,\n    'side_x': side_x,\n    'side_y': side_y,\n    'timestep_respacing': timestep_respacing,\n    'diffusion_steps': diffusion_steps,\n    'animation_mode': animation_mode,\n    'video_init_path': video_init_path,\n    'extract_nth_frame': extract_nth_frame,\n    'video_init_seed_continuity': video_init_seed_continuity,\n    'key_frames': key_frames,\n    'max_frames': max_frames if animation_mode != \"None\" else 1,\n    'interp_spline': interp_spline,\n    'start_frame': start_frame,\n    'angle': angle,\n    'zoom': zoom,\n    'translation_x': translation_x,\n    'translation_y': translation_y,\n    'translation_z': translation_z,\n    'rotation_3d_x': rotation_3d_x,\n    'rotation_3d_y': rotation_3d_y,\n    'rotation_3d_z': rotation_3d_z,\n    'midas_depth_model': midas_depth_model,\n    'midas_weight': midas_weight,\n    'near_plane': near_plane,\n    'far_plane': far_plane,\n    'fov': fov,\n    'padding_mode': padding_mode,\n    'sampling_mode': sampling_mode,\n    'angle_series':angle_series,\n    'zoom_series':zoom_series,\n    'translation_x_series':translation_x_series,\n    'translation_y_series':translation_y_series,\n    'translation_z_series':translation_z_series,\n    'rotation_3d_x_series':rotation_3d_x_series,\n    'rotation_3d_y_series':rotation_3d_y_series,\n    'rotation_3d_z_series':rotation_3d_z_series,\n    'frames_scale': frames_scale,\n    'calc_frames_skip_steps': calc_frames_skip_steps,\n    'skip_step_ratio': skip_step_ratio,\n    'calc_frames_skip_steps': calc_frames_skip_steps,\n    'text_prompts': text_prompts,\n    'image_prompts': image_prompts,\n    'cut_overview': eval(cut_overview),\n    'cut_innercut': eval(cut_innercut),\n    'cut_ic_pow': eval(cut_ic_pow),\n    'cut_icgray_p': eval(cut_icgray_p),\n    'intermediate_saves': intermediate_saves,\n    'intermediates_in_subfolder': intermediates_in_subfolder,\n    'steps_per_checkpoint': steps_per_checkpoint,\n    'perlin_init': perlin_init,\n    'perlin_mode': perlin_mode,\n    'set_seed': set_seed,\n    'eta': eta,\n    'clamp_grad': clamp_grad,\n    'clamp_max': clamp_max,\n    'skip_augs': skip_augs,\n    'randomize_class': randomize_class,\n    'clip_denoised': clip_denoised,\n    'fuzzy_prompt': fuzzy_prompt,\n    'rand_mag': rand_mag,\n    'turbo_mode':turbo_mode,\n    'turbo_steps':turbo_steps,\n    'turbo_preroll':turbo_preroll,\n    'use_vertical_symmetry': use_vertical_symmetry,\n    'use_horizontal_symmetry': use_horizontal_symmetry,\n    'transformation_percent': transformation_percent,\n    #video init settings\n    'video_init_steps': video_init_steps,\n    'video_init_clip_guidance_scale': video_init_clip_guidance_scale,\n    'video_init_tv_scale': video_init_tv_scale,\n    'video_init_range_scale': video_init_range_scale,\n    'video_init_sat_scale': video_init_sat_scale,\n    'video_init_cutn_batches': video_init_cutn_batches,\n    'video_init_skip_steps': video_init_skip_steps,\n    'video_init_frames_scale': video_init_frames_scale,\n    'video_init_frames_skip_steps': video_init_frames_skip_steps,\n    #warp settings\n    'video_init_flow_warp':video_init_flow_warp,\n    'video_init_flow_blend':video_init_flow_blend,\n    'video_init_check_consistency':video_init_check_consistency,\n    'video_init_blend_mode':video_init_blend_mode\n}\n\nif animation_mode == 'Video Input':\n    # This isn't great in terms of what will get saved to the settings.. but it should work.\n    args['steps'] = args['video_init_steps']\n    args['clip_guidance_scale'] = args['video_init_clip_guidance_scale']\n    args['tv_scale'] = args['video_init_tv_scale']\n    args['range_scale'] = args['video_init_range_scale']\n    args['sat_scale'] = args['video_init_sat_scale']\n    args['cutn_batches'] = args['video_init_cutn_batches']\n    args['skip_steps'] = args['video_init_skip_steps']\n    args['frames_scale'] = args['video_init_frames_scale']\n    args['frames_skip_steps'] = args['video_init_frames_skip_steps']\n\nargs = SimpleNamespace(**args)\n\nprint('Prepping model...')\nmodel, diffusion = create_model_and_diffusion(**model_config)\nif diffusion_model == 'custom':\n    model.load_state_dict(torch.load(custom_path, map_location='cpu'))\nelif diffusion_model == 'FeiArt_Handpainted_CG_Diffusion':\n    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'),strict=False)\nelse:\n    model.load_state_dict(torch.load(f'{model_path}/{get_model_filename(diffusion_model)}', map_location='cpu'))\nmodel.requires_grad_(False).eval().to(device)\nfor name, param in model.named_parameters():\n    if 'qkv' in name or 'norm' in name or 'proj' in name:\n        param.requires_grad_()\nif model_config['use_fp16']:\n    model.convert_to_fp16()\n\ngc.collect()\ntorch.cuda.empty_cache()\ntry:\n    do_run()\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print('Seed used:', seed)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# workaround for data_subdirs not refreshing in draft session, switched to FileLinks\nimport os\nprint(f'\\nFileLinks:\\n')\nfrom IPython.display import FileLinks\nFileLinks('images_out/.') # link to all files in /","metadata":{"_uuid":"daf98f41-4819-40ac-a1c9-8f4eb818f542","_cell_guid":"f57d1a9f-c496-4dcf-8e25-0857d1cf6505","id":"DoTheRun","execution":{"iopub.status.busy":"2022-10-18T23:58:41.114763Z","iopub.execute_input":"2022-10-18T23:58:41.115014Z","iopub.status.idle":"2022-10-19T00:41:49.553606Z","shell.execute_reply.started":"2022-10-18T23:58:41.114991Z","shell.execute_reply":"2022-10-19T00:41:49.552479Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc3d43291c24921a34e361e0f8da8b3"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0155df614d545a6ae567528fc680af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Steps:   0%|          | 0/240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060e6bcda95b47289812f4d77b0a99e3"}},"metadata":{}},{"name":"stdout","text":"Seed used: 1557011537\n\nFileLinks:\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"images_out/./FloralTest/\n  FloralTest(0)_0.png\n  FloralTest(0)_settings.txt","text/html":"images_out/./FloralTest/<br>\n&nbsp;&nbsp;<a href='images_out/./FloralTest/FloralTest(0)_0.png' target='_blank'>FloralTest(0)_0.png</a><br>\n&nbsp;&nbsp;<a href='images_out/./FloralTest/FloralTest(0)_settings.txt' target='_blank'>FloralTest(0)_settings.txt</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}